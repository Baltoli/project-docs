# Part III Project Work Log & Notes

## 24/11/16

* Build TESLA on both remote and local machines - it would be good to be able to
  work on both (if ssh is slow or compile times are a problem).
  * Local - ran into a problem when building. Possibly because we require msan
    for something? Can't get it to build for now - try again later if really
    necessary.
  * Remote - seems to build *a lot* faster than on my laptop. Needs libprotobuf
    installed. Possibly better to wait for cluster access rather than mucking
    around in my home directory to try and install it?

* Cluster login - have been added to the appropriate gorups but need to find out
  how to actually log in / install software / etc.

* Reading code and understanding the structure of TESLA
  * Analyzer - tool for walking over TESLA assertions in a program and
    serializing them to protobuf
    * Main entry point in analyzer/tool.cpp parses command line options etc, then
      builds a clang tool and calls out to it
      * Clang tool takes a compilation database and a list of source paths to work
        on
      * Uses a TeslaActionFactory as an argument to the tool - this builds a
        TeslaAction that runs over the source files. Serializes automata usages to
        a protobuf file / string
    * Vistor class walks over automata descriptions and tries to parse them
  * Common - functionality used by multiple parts of TESLA
    * Class that describes the internal automaton / states / transitions between
    * Protocol buffer specification
    * Automatic name generation for internal components
    * Manifest describes the TESLA instrumentation to perform - can be loaded
      from a file (is this the same format that's dumped out by the analysis
      phase / catted together?)
    * Convenience stuff for debugging
  * Instrumenter -  does the work of adding stuff to compiled IR based on the
    analysis phase
    * Adds to the IR based on the actual TESLA assertions that are made
    * Field reference, function calls, struct assignment, assertions etc are
      defined in this part of the code base
    * Translation from actual program events into automata descriptions
  * Tools - the TESLA binary tools that make up the workflow as described in the
    project documentation
    * `cat` - reads all the automata from the filenames passed in, then combines
      them together into a single manifest (doing error checking - for example,
      two automata with the same name but different contents is an error).
    * `print` - just prints out an automata description to the console in a
      bunch of different formats
    * `get-triple` - just a call to the LLVM function that gets the system
      triple.
  * Tests - for automata, instrumentation, integration, parsing, regression
    * Parsing tests put TESLA assertions into a bunch of different C programs
      and use FileCheck to ensure that the assertion grammar is parsed out
      correctly
    * Integration tests build full example programs and ensure that TESLA
      behaves correctly
    * Regression tests cover examples generated from previously found bugs in
      the system
    * Automata covers working on automata protobufs (either written out
      explicitly or generated from C programs) - e.g. that they can be
      concatenated together & resolved properly etc.
    * Instrumentation tests the interaction of the system with LLVM bitcode
      (i.e. that the correct checks are being added to the bitcode files)

* TESLA workflow summary / idea of how data flows through the system
  * Starts from ordinary C programs with no assertions in them
  * Then they are instrumented by adding TESLA assertions to check some
    property of system behaviour
  * A C file with TESLA instrumentation is then *analysed* by the first part of
    the TESLA workflow - the analysis phase produces a `.tesla` file which
    contains an automaton description (these descriptions contain essentially an
    AST of the assertions contained in the program - examples are in the tests
    directory as FileCheck lines).
  * The C file is also compiled as normal into a .ll file using clang once it
    has been analysed.
  * All the .tesla files generated by the analysis phase are combined together
    using the `cat` utility into one manifest.
  * Then, the single combined manifest is used to instrument the compiled LLVM
    bitcode files (giving a set of augmented LLVM bitcode files).
  * The instrumented files can then be compiled using `llc` into a binary as
    normal, with the TESLA functionality included.

## 2/12/2016

* Got set up on the development server by Khilan - managed to get TESLA building
  and installing into a home directory appropriately.
* Manually worked through compiling a program to use TESLA etc.
* Built a somewhat hacky CMakeLists that lets me compile a _simply-structured_ C
  application into a version that is instrumented with TESLA. At some point will
  need to look into how this can actually be done in practice for a larger code
  base! (Ask Jon Anderson?)
* Begun to look into building some very simple TESLA assertions on a small
  program.

## 3/12/2016

* Continued to work on very simple examples learning to use TESLA. Found that
  the documentation is out of date in a number of places:
  * `TESLA_NOW` is replaced by `TESLA_ASSERTION_SITE`
  * `called` seems to have been replaced by `call` but I'm not sure if this is a
    drop-in replacement.
* Notes on using TESLA:
  * `TESLA_ASSERTION_SITE` refers to the point at which the assertion is written
    in the original program (hence why `previously` and `eventually` are written
    the way they are). It can be used to express more complex interleavings of
    temporal properties by having things before / after it as well.
  * `ANY` takes an argument (`int`, `long long`, `ptr` etc.) to represent the
    type of argument being passed to the function call.
  * The `call` function takes a single parameter - the function call as it would
    be written in the program! This is different to `returnfrom`, which takes an
    extra parameter for the return value. Why can't you do
    `call(foo(ANY(int)))`? Ask Jon.

## 5/12/2016

* Did some more digging into why explicit automata descriptions aren't working
  for me. No real answers yet - the symptom is that I have an undefined name in
  the instrumented LLVM code. Something has inserted a global variable for the
  automaton into the code but has not "expanded" it with a definition. Have
  emailed Jon for clarification / help but no answer yet. The next place to look
  is probably trying to find out what is meant to replace the global variable.

## 15/12/2016

* Still no reply from Jon. Should get Robert to prod him. Looking into why I get
  these linker errors with a much-simplified example. Seems that the
  `tesla_update_state` function is getting called with `my_auto` as an argument,
  which is a global variable in the LLVM IR. This means (probably) that
  something isn't actually populating this global variable with a real value.
  Dig though code and find out where?
* When we end up calling `ExternAutomatonDescrip` to get a global variable,
  the first time we call, there isn't a variable ready. The second time, there
  is and we get it.
* Looks like `BuildAutomatonDescription` is doing what we want but it isn't
  getting called - it is responsible for getting rid of extern specified global
  variables and replacing them with things that actually have an initializer!
* It seems to be called in two separate places in Assertion.cpp - how do these
  different places get triggered? Actually only in one location for the version
  of TESLA I have on my machine (why is this different??) This was different
  because I was building against CTSRD TESLA rather than CADETS TESLA and there
  was some difference in the code for newer commits.
* The path that gets to where we want to be is returning early because there are
  no assertions in the module. How do I add an assertion to the module?
* `TESLA_STRUCT_AUTOMATON` is dead - the actual syntax is to use the automaton
  just like a function call (for example, `eventually(my_auto(a))`)
* Usage: you can't go arbitrarily far down the call stack using `TESLA_WITHIN` -
  it seems to be only within the current function that it's usable. 
* Setting up: managed to get my tesla experimentation work shoved into the TESLA
  source tree so that I can build etc all in the same place. Took a bit of
  working to get CMake not to break but now it works. Next step: modelling
  simple locks.

## 16/12/2016

* Begin to work on getting a very simple model of locks verified. Set up project
  structure etc. Convenience macro for TESLA struct field accesses (might be
  extensible in the future to a full struct macro for ease).
* Implemented a simple lock automaton that ensures that a particular function is
  'well-behaved' with regard to a particular lock (i.e. it acquires the lock at
  some point, then releases it later). This currently catches things like
  releasing without acquiring, acquiring after release. It can't handle a
  deadlock situation yet (can it?). 
* Need to look into why if a thread exits without calling `lock_release`, it
  won't trigger an assertion failure.
* Using previously instead of eventually seems to fix the deadlock problem on
  the lock automata. Also added a check to make sure that acquire calls go
  "F...FT" only.
* Using two or more locks: I think strict mode might be a problem here. How to
  fix this?

## 17/12/2016

* Looking into how I can use `tesla-print` to get `.dot` files for debugging.
  Managed to get graphviz on OS X to output me a file. Workflow is a little bit
  inconvenient though (but obviously no way to view pngs on the server).
* Currently doesn't look like there's an easy way to have a function that acts
  as a thread worker be instrumented from outside of its own definition (or a
  caller), nor for a function to work on *multiple* locks.
* Even shifting stuff one level up the call stack seems to break things a bit -
  why can't I use `acq_rel` from `thread_work` which just calls into
  `thread_say`?
* Seem to have sorted this out - if I use the wider bound of `thread_work` then
  it works, but not the narrow bound of `thread_say`.
* Now have a system for making assertions about several locks in the same
  function (not interleaved though I don't think). Uses a strict sequence.
* At this stage I have a working verifier for a very simple lock that can be
  used to verify acquire-release behaviour for a single lock, multiple locks in
  sequence, or multiple locks interleaved.
* Next step is to think about how best to statically analyse this type of
  structure, and also to look at how locks are implemented internally in (for
  example) FreeBSD (and then to come up with equivalent automata for the real
  locks).
* Also worth thinking about what properties (in a formal sense) this
  implementation allows me to verify.

## 18/12/2016

* Small amount of work looking into the lifetime automata but to no real
  progress - I can get them to compile but their definitions seem to somehow be
  different to regular 'in-function' automata. Shelving this for now.

## 19/12/2016

* Begin looking into how static analysis can be performed on TESLA automata,
  starting with the example of a single lock being acquired and released.
* Structure of a TESLA automaton is described at the highest level by the
  protocol buffer specification ultimately. This gives us the "grammar" by which
  an automaton is described. Can see examples of this as the ouput from the
  `tesla-cat` step of building a program against TESLA.
* Obviously code for working with an automaton in this state is generated by the
  protobuf build step (so should look up how to actually work with this
  generated code).
* A `Manifest` describes all the automata contained in a single file, while an
  `Automaton` is singular (only describes one single set of states and
  transitions).
* Initial effort will be to statically analyse the locking assertions I've built
  over the last few days.
* What do we want to prove about this automata specifically? (in this case I
  think that I do actually want to be specific and see how it can be generalised
  to a more significant structure).
  * The `acq_rel` automaton *should* be generic over any kind of lock simple
    enough to be described with simple atomic acquire / release semantics.
  * For example, a Pthreads mutex could be adapted to this structure by wrapping
    the Pthreads acquire / release functions (should try this at some point by
    making an *almost* identical example to the current `locks.c` experiment.
  * What `acq_rel` asserts is that on any code path, the function will spin
    around 0 or more times failing to acquire the lock, then successfully
    acquires the lock, then releases it.
  * Need to characterise *exactly* what behaviour is allowed and forbidden by
    the automaton as it is currently implemented (by building a set of smaller
    test cases that demonstrate its behaviour - note that `lock_acquire` and
    `lock_free` are *mockable* - they present an interface that can just as well
    be implemented by something that doesn't actually do any locking at all!).
  * Then once I have an informal but demonstrable model of the lock automaton's
    behaviour, I can start to work out exactly how these properties can be
    described (and proved) at the IR level.
  * For this specific analysis of my automaton only, there is no need to really
    work with the TESLA IR at all - instead need to just look for the very
    specific form that instruments an acquire / release cycle.
* Worked out some CMake hackery to allow multiple TESLA executables to share a
  single source file. As far as I can see the solution of building every source
  file for every target executable is necessary - instrumentation will change
  things etc so this is the only way to do it generically (without having custom
  TESLA invocations for every single executable being built). These improvements
  to the TESLA build system allow me to build arbitrary C executables with TESLA
  assertions built in just by calling `add_tesla_executable`.
* Have also begun to do some rebasing and cleanup on the locks branch so that
  when I merge it to master it's not totally horrific to look at.
* Implemented a few small misbehaving programs to demonstrate the kinds of
  behaviour that the lock automata attempt to catch.

## 20/12/2016

* Finished off writing example programs that show the behaviour of my simple
  lock automata (successful / good path as well as several 'failed' examples).
  Still running into the issue that assertions don't seem to care about the
  identity of the pointer they're being called with (`acq_rel(&lock)` is
  equivalent to `acq_rel(&other)` behaviourally - it doesn't seem to matter
  which one is actually acquired and released). Need to work out if there's a
  solution to this (my instinct is that there isn't an easy way to do so because
  of the way instrumentation is added to code).
* One possible hacky workaround is to add a global ID to every lock - then, the
  `lock_X` functions could work on a lock ID rather than directly on a pointer.
  I think this might be easier to reason about, provided that the correct
  guarantees are made of the global lock ID 'repository'.
  * `lock_init` would need to increment the global ID counter, set the field on
    the lock, put the lock itself into an array (by value), then return the id.
  * `lock_acquire` and `lock_release` would need to work on the base pointer +
    offset, rather than on the addresses directly. They could then be modified
    to take an ID (integer) rather than a direct pointer.
  * `lock_free` would then need to perform any necessary cleanup (can't see what
    this would actually be just now).
  * This would mean that initializing a lock would not necessarily be atomic
    (but that's fine I think - just need to do lifecycle things sequentially
    before using locks for real).
  * Doing this might even make it easier to statically analyse locks - working
    out the value of an integer value seems like it would be more amenable than
    chasing pointers.
* Update: this strategy of using IDs rather than pointers isn't totally workable
  with the current implementation. Maybe possible in the future to rip it out
  and design from scratch (but wouldn't be able to use automata quite as
  idiomatically as they currently are).
* So with that in mind, the properties we can observe the `acq_rel` automaton as
  having are (for a *single* lock being used by a function):
  * `lock_acquire` returns false 0 or more times, until it returns true exactly
    once. It is not called again.
  * `lock_release` is called exactly once.
  * Any extra calls to either lock function will fail.
  * Calling `lock_release` before `lock_acquire` has returned true will fail.
* Where in the TESLA workflow should static analysis live?
  * Likely to be in the form of an IR pass to actually perform the analysis (but
    also need some information from the analyser level to know where to apply
    analysis).
  * For now, the analysis will be specific to the `acq_rel` automaton, and so
    all we need to do at the analyser level is find usages / bounds / locks etc
    for each instance of the automaton.
  * So at the point at which static analysis is performed we require:
    * The combined manifest file with the whole-program view of assertions made
    * The uninstrumented `.ll` file that the program was compiled into
  * From these prerequisites, we should then produce a statically-analysed
    version of the manifest that has had verifiable assertions removed (but that
    remains in the same format).
  * The optimised manifest can then be used as a drop-in replacement for the
    non-optimised version (bonus from this is that the program can then be
    instrumented using both versions and their behaviour compares).
* Gist of this is that what we want is really a new TESLA tool (`tesla-static`)
  that takes an bitcode file and a manifest for analysis, then returns a new
  with possibly different contents.
  * Benefit of this is that we're not doing anything unsafe at the bitcode level
    (in this particular model). All we do there is analysis of some kind that
    informs the different manifest.
  * Probably need this to be whole-program analysis. Should therefore
    investigate whether `llvm-link` can be built into the build system (i.e.
    that it plays nicely with instrumentation etc.)
  * It can in fact. I've now updated the TESLA build script to link all the .bc
    files into a single one before instrumenting (and have changed extensions
    etc. to be more correct). Now the TESLA build script will compile the
    program from a single .bc file (and analysis can be done on this
    individually).

## 21/12/2016

* Got the boilerplate set up for the static analysis tool (very similar to
  `tesla-instrument`). It is now able to load a manifest and an IR file from
  disk when it is run.
* Did a bit of tinkering to see what information I can get out of the manifest
  file using the protobuf generated code etc. Seems to be a reasonably easy
  interface to work with (`set_X` for setting fields, `X` for getting fields).
* Now need to work out what kind of interface I want the static analysis to
  have.
  * Will make sense to have a modular system (so that future analyses aren't
    tied directly into the structure of the tool itself and the initial work I'm
    doing here).
  * Then each additional analysis I do can be plugged into the tool with a
    minimum of work.
  * The information that we have at the start is the unmodified manifest file
    and the IR. It makes sense that each 'pass' will get the same things passed
    to it, with the exception that the manifest will be changed between each
    step (as it wouldn't make sense to leave it unchanged and then do every pass
    at once at the end).
  * So it will make sense to have:
    * Base class for manifest passes that defines an interface each conforms to.
    * Builder class that takes a list of instantiated manifest passes and runs
      them one after another in a pipeline.
* Have implemented a pass system that allows for manifest passes to be
  constructed and run on manifests / IR files.
* Now the next problem is how to create a new manifest given the original one.
  Seems to be that modifying in place isn't possible as the `Manifest` class is
  const-ed up pretty heavily. Should look further at `tesla-cat` to work out how
  this could be done. Looks like `ManifestPass::run()` will need to actually
  return a new `Manifest` (which can be passed into the next one in the pipeline
  etc.).
* The analysis for `acq_rel` is an include / exclude decision. So what needs to
  be done is walk the root automata for things that are `acq_rel` (if it's
  included), then delete (don't pass on) root automata if the analysis succeeds.

## 22/12/2016

* The `cat` tool generates a new file by directly writing the textual
  representation of a `ManifestFile` to disk. It seems to be valid to initialize
  a `Manifest` only given the file containing the textual representation.
* The logic to do this is pretty complex, but what we're going to want to do is
  hijack the process at the point at which we have a buffer containing a
  `ManifestFile` protobuf object.
* By doing this we then have a workflow:
    immutable Manifest --analysis-> ManifestFile --load--> new Manifest
* This is then composable for many passes in sequence, and `tesla-cat` shows how
  to generate a `ManifestFile` given constituent parts (Automata and Usages).
  From this we can then implement the recursive walk accept / reject style that
  will be used for the `acq_rel` analysis.
* Have replaced some parts of the `Manifest` code with uses of
  `std::unique_ptr`, as the legacy LLVM reimplementation has been deprecated. It
  would be worthwhile to fix up the rest of the code.
* Also another quality of life problem - there's a problem with how `llvm-lit`
  is running tests (to do with an output option), so probably wise to fix that
  before I do any major reshuffling of TESLA internals.
* So now we have a way to construct a `Manifest` from a `ManifestFile`. We don't
  have direct access to the input `ManifestFile`, but what we do have is the set
  of defined Automata and the set of root Usages. We probably don't want to ever
  get rid of a defined automata (as if there's no references to it then it will
  disappear at instrumentation time).
* That means that we need to copy all the defined Automata into a new
  `ManifestFile` regardless of what else is being done.
* Once the definitions are copied, it's then a case of walking root usages (how
  to handle sub-automata). This is at least the case for the include / exclude
  decision we're making in this model at first.
* So a the run method of each pass will actually need to return a new manifest
  (pointer to).
* Hygiene: pass interface is *bad* at the moment. Should rewrite to eliminate
  usage of raw pointers (think about ownership semantics). Fixed this for the
  most part, but there's still the interface between `main` and
  `ManifestPassManager` that has a couple of `shared_ptrs` sticking around.
  Should work to eliminate those as well.
* At the moment, a manager runs passes and resets its own `Manifest` (this
  allows for an 'ownership chain' through the run function). Should also maybe
  clean up the external interface a bit as everything is public at the moment.
* Next up it looks like some kind of generic way to walk over the *usages* in a
  manifest and make some kind of decision about them. Obviously the protocol
  buffer format gives us a handy way of describing the manifest as a tree, but
  what would be the best way to iterate over it?
* Alternatively, don't do it generically for now. The `acq_rel` automaton should
  be fairly easy to recognise usages for. We want roots that:
  * Are a sequence of events beginning with the assertion site, immediately
    followed by a subautomaton with name `acq_rel`, followed by a return from
    some function.
  * Can recognise things in this format by doing some pretty nasty protobuf
    code, but it's probably simpler for now to do that than to wrangle up some
    crazy generic method to do it.
  * Then once we have a way of recognizing usages of `acq_rel`, the next step is
    to pick out the bounds of each usage and do the analysis on the IR.

## 23/12/2016

* Today: work on recognizing usages of `acq_rel` in an automaton root. Want a
  function that takes a root usage, and returns true if it is an instance of
  `acq_rel`.
* Came across an interesting problem where instrumenting a module fails when all
  of the roots are removed from the file (fails in InstrContext.cpp with an
  error relating to automaton lifetimes).
* Have now built the static analysis tool into the TESLA CMake build
  (optionally). It can now automatically run the static analyser over the
  genrated manifest and compile two versions of the binary.
* So we actually need to look at all of the automata descriptions first off, and
  identify which automata use `acq_rel` (rather than looking directly at the
  usages as I previously thought).
* Strategy: search for automata that reference `acq_rel` in the way we define
  (sequence of events given above). Then populate the vector of locations
  appropriately. From these locations, iterate through the roots and check each
  location for membership in the set. If it is in the set, flag the beginning
  and end locations for analysis.
* So now have a pass that manages to remove *any* usages of `acq_rel` that it
  finds, but it still has the problem from above (that lifetimes get messed up).
* Should also work out a better way of getting myself out of a 'trusting trust'
  cycle. If work on `tesla-static` introduces a bug, then things start to fail
  *badly* at build time because it's part of the build process. Intermediate
  could be to use the CMake binary dir version?
* Actual solution turned out to be to use an extra CMake config variable to
  toggle static analysis of built binaries on or off. So if `tesla-static`
  introduces a build error, the solution is now to run `soff` so that the broken
  binary doesn't get run.
* Is the lifetime error limited to when there are no roots at all in a manifest?
  Confirmed that this seems to be the problem. Rather than spleunking through
  the original TESLA code, it might in fact be easier to do a cleanup pass that
  finds unused automata in the manifest and deletes them.
* The problem then with just deleting them is that calls to
  `tesla_inline_assertion` will then fail. Need to think about how best to deal
  with this problem going forward. Some options:
  * Extend protocol grammar to mark usages as deleted, then ignore them at
    instrumentation time, leaving the rest of the machinery intact.
  * Move away from manifest pass structure for now and use regular LLVM
    machinery to achieve the same idea.
* Worth nothing that currently I still don't know the actual cause of the
  lifetime problem.
* Final thoughts: I think the deletion marker is the best way to go without
  touching an LLVM pass as well. Benefits are:
  * No more messing around with these weird lifetime errors
  * Not throwing away information as the current approach does - all it's doing
    is *adding to* the manifest data to suggest approaches to the
    instrumentation pass.

## 13/1/2017

* Back working on TESLA after a break to do coursework post-christmas. Meeting
  with Robert earlier today in which we discussed the work that I've been doing
  so far on the project. Main points were:
  * He agrees that the problem of not being able to use the same automata on two
    different pointers is perplexing - should do some more digging into this and
    work out the actual source of the problem. Mentioned a table of parameters
    that can be indexed into to find the correct automaton instance for a call.
    This code might be in the runtime library.
  * Modelling the automaton using function calls to acquire and release is
    probably best as it abstracts the lock internals from the semantics of the
    action.
  * Think about how the model devised could be broadened to (for example)
    reader-writer locks, or data structures that permit operations only when
    locked.
  * Intra-procedural analysis and the placement of `tesla static` seem to be
    reasonably well thought out for now, as well as the proposed extensions to
    the instrumenter and the protobuf structure.
* Next steps to think about for the project:
  * Work out how to do the data flow analysis for simple acquire / release locks
    as I've implemented (will involve writing an LLVM pass).
  * Related: look into updating the protocol buffer specification so that I can
    add my own fields for the instrumenter to look at.
  * Should probably split out sections of this log that are too 'stream of
    consciousness' into separate documents (designing components like the data
    flow analysis is a good candidate).
  * Look into the problem / bug / misunderstanding relating to using multiple
    pointers with the same automaton. If no solution then ping Jon on Slack to
    get his thoughts.
* Progress today:
  * Fixed the systematic errors with the test suite, and have identified a
    possible regression in the parser.
  * Parser regression notes:
    * ParseArg -> MemberExpr branch -> ParseStructField -> ParseArg -> ???
    * Need to figure out what the next branch is!
    * Left a TODO in the right place to pick up where the error seems to
      manifest itself first. Should look at why the incorrect branch makes two
      goes round the function!

## 14/1/2017

* Fixed the parser regression identified yesterday. The solution was to add an
  extra parameter to the argument parsing methods so that they can be
  specialised to the case where we're passing a whole structure by value, but
  only care about a single field.
* Next step for this afternoon's work: look into the multiple automaton stuff
  and see if I can get that to work.
* Looked into strict / conditional mode in more detail. Think I have worked out
  (at least partially) what the problem with using multiple locks is (related to
  strict mode).
* Wrote up findings for the strict / conditional mode stuff.
* Starting to look at lock behaviour but have run into another parser issue (I
  think, anyway) - indirection seems to be be going wrong for me.
* Another useful command would be to disable experiments in general (eon / eoff)
  so that TESLA can be rebuilt and installed easily if the experiment build is
  broken, without having to change code.

## 15/1/2017

* Added the experimental on / off flags that allow TESLA to be rebuilt and
  installed if the experiments are breaking the build.
* Looked into the indirection issue from yesterday, which seems like it could be
  a known limitation - sent Jon a message asking for clarification.
* Wrote up current status of locking automata and informal / behavioural
  specifications of the properties that it asserts.
* Integrated the new protocol buffer field into `AcquireReleasePass`. As a first
  / dummy operation it just searches for usages of the automaton named, then
  adds the deleted field before copying the usage across.
* Next step is to design the actual optimisation that informs the copy yes / no
  stage of the pass.
* Extended the instrumenter so that it will skip over deleted usages without
  adding any extra instrumentation. Tested this on the locks experiments, and in
  simple cases it seems to work (allows the failing cases to pass when their
  usages of `acq_rel` are deleted).
* Looked into the usage of field assignments as assertion bound events.

## 16/1/2017

* Looked into how the variable names in an assertion site are mapped onto LLVM
  `Value` objects. Found the mechanism that does this by searching all of the
  things in scope at the assertion site (allocas, globals and parameters).
* Worked out where the actual static analysis will live inside the
  `AcquireReleasePass`, and when it will get called.
* Started to stub out some implementations / things that will need to be done
  to get the appropriate information out of the usage and module.
* Fixed that annoying CMake install problem for the experiments directory.

## 17/1/2017

* Next step is to rip out the part of the instrumenter that maps variable names
  onto an LLVM `Value`. Have implemented this and moved the appropriate parts
  out into a common location so that the static analyser is able to access them.
* Working on integrating the LLVM pass boilerplate into the setup I have going
  at the moment - have it such that the pass is run, setting a variable on
  itself that is checked by the consumer of the pass afterwards.
* Realised that my pass construction interface isn't quite right, and I need to
  actually give an `Automata`.

## 18/1/2017

* Rewired stuff to pass an `Automaton` instance into the LLVM pass so that the
  argument information can be accessed.
* Run into a problem with how arguments are stored on an `Automaton` - need to
  recurse down into subautomata.
* Worked it out so that the LLVM pass can be given the argument list from the
  subautomata. In the future (i.e. when thinking about generic analysis), it
  will be useful to work out a way of extracting the argument vector from an
  automaton's subautomata more generally. Given that each has an index, I can
  see it being possible via a subautomata walking approach.
* Implemented the first actual static analysis of the project! I can now check
  `acq_rel` assertions to make sure that on every call path in the bounds, calls
  to the acquire and release functions *only* use the variable named in the
  assertion.

## 19/1/2017

*  Big achievement for the day is getting the project to build without errors on
   LLVM 3.4. It doesn't look like it'll be easy to get it updated further
   because of incompatibilities with the CMake system that I haven't been able
   to resolve yet.

## 23/1/2017

* Worked out how to get call graph information out of the corresponding LLVM
  pass.
* Realised that some of my thinking on how to improve the 'other lock' analysis
  was actually wrong.
* Begun to implement a more general framework for implementing static analyses
  within the context of an LLVM pass. The idea is that the LLVM pass has a
  larger-scale analysis it wants to do, and the individual analyses do the
  smaller units of work.
* Added some TODOs and questions about the static analysis methodology, and the
  `OtherLock` analysis.

## 24/1/2017

* Finished factoring out the "Other Lock Used" static analysis into its own
  self-contained implementation. The next step will be to write more of these
  analyses.
* Added a new example that doesn't check that acquisition was successful.
* Begun to work on the analysis that will check for non-branching on the result
  of calls to the acquire function.
* Maybe worth noting that because the `acq_rel` analysis is so specific to this
  one automaton, it is possible to generate warnings.
* Found a limitation of the interprocedural approach taken here - can't really
  go back to a file name once a debug location is identified.
* Finished implementing the "no branch" analysis.
* Added a failing example for the case when a lock is released before being
  acquired, and begun to work on the analysis of this behaviour.

## 25/1/2017

* Today working on the analysis that checks for release-before-acquire of a
  lock. Implemented this and seen it working on the example written yesterday to
  demonstrate the problem.
* Implemented a common way to get the debug location from an instruction
  (module, function, line), and converted existing analyses to use it.
* Spent some time in debugging hell trying to get the CallGraph to work, only to
  discover it was all my fault in the end anyway. Or is it? Still getting
  bizarre results.
* Maybe worth reinventing the wheel after all and building a 'SimpleCallGraph'
  structure that can just build itself from a module. Saves lots of time
  hassling the LLVM stuff to work.
* Begun to work on this data structure to save on LLVM pain.

## 26/1/2017

* Implemented the simpler call graph structure and started to use it in the call
  order analysis.
* Realised that my approach to this is actually a bit wrong and have had to
  think about how to improve it (problem is ordering, really).

## 27/1/2017

* Rethought some of the problems identified yesterday (with dominance vs. call
  graph analysis), and realised that it is possible to reconcile the two into a
  useful analysis.
* Meeting with Robert - progress continues and some interesting ideas and
  thoughts have been written up as notes.
* Finalised the call order analysis with a useful warning message.

## 28/1/2017

* Noted that using a lock asserted about only once is covered by the RBA
  analysis.
* Realised that I'm missing some very simple analyses that will catch mistakes
  in usage (e.g. missing calls, release dominance).
* Beginning to implement these analyses to cover all the examples.
* Worked out that what I want is reachability rather than dominance in the cases
  I've used so far.
* Begun to implement a way of computing BB reachability within a function.

## 29/1/2017

* Implemented the reachability graph check for getting from one basic block to
  another.
* Finished implementing the release reachability analysis that makes sure we
  can't call release again after calling it once.
* Begun to look at an analysis that looks for address-taken instances of the
  acquire and release functions.
* Looked into some informal performance analysis of optimised apps - learned to
  use gprof for this. Needs a more formal analysis to take into account the
  large variation in results, but generally can see an improvement when using
  the optimised version. A next step would be to fix an actual benchmark and get
  some results into a graph etc.

## 30/1/2017

* Identified that the multiple acquire example actually does need the FF...T
  analysis example to be recognised (as before it was in a weaker form that
  could be detected by the no-branch analysis).
* Implemented a chunk of the tracing algorithm that will eventually map calls to
  acquire onto a branch, with a modifier to choose the destination.
* Finished implementing the FF...T analysis, and thought about some bugs that
  are picked up by this that inform a couple more small analyses.
* Started to think about benchmarking and profiling an example to show the
  benefit of removing locks.

## 31/1/2017

* Looked more into how callgrind can be used to get program costs - should
  confirm this with Robert as a viable benchmarking method. Collected the data
  from the callgrind runs for 10 threads.

## 3/2/2017

* Meeting with Robert - chatted about benchmarking strategies for synthetic
  examples for the mutex analysis. Callgrind seems viable for simple examples,
  but for more complex ones it might affect the concurrent execution too much to
  be useful. For a better analysis, a real computation might well provide a
  useful measure via wall clock time (rather than just spinning).
* Asked Jon where to get an updated FreeBSD kernel with TESLA assertions so that
  I can try to reproduce the performance analyses from the original paper.
* Begun to have a think about how we can do more generic analysis of automata,
  including a small writeup of the key things we'll need to look at and some
  questions that should be answered.
* Chatted to Arun about building FreeBSD with TESLA enabled.
* Starting to look into compiling a FReeBSD kernel that I can add things to and
  instrument etc.
* Ran into some errors when compiling FreeBSD.

## 4/2/2017

* Added methods to the Manifest class that allow for automata search that
  doesn't panic if they aren't found. Updated current code to use these new
  methods if possible.
* Fixed some bugs in the acq-rel analysis that caused crashes when not running
  on the expected examples.
* Started to think about call sequence analysis.

## 5/2/2017

* Trying to build the CADETS version of the kernel rather than the TESLA
  version. The problem I was experiencing yesterday seemed to be to do with
  there not being a rule to make a particular .tesla file for a .c file, so
  maybe have a look at the makefile that gives the rules for TESLA builds, as
  well as the one that builds cam.c.
* Worked out some properties of functions that are useful to prove to perform
  static analysis (calls exactly once etc).
* So it's definitely TESLA causing the problem with the kernel build - the
  vanilla CADETS version builds just fine.
* Implemented some auxiliary stuff getting towards a calls-exactly-once
  analysis.
* Finished implementing the calls-exactly-once analysis.
* Did a lot more digging into the Makefile problems that are stopping me from
  being able to build the kernel with TESLA enabled.

## 6/2/2017

* Spent lots of time tracking down an annoyingly subtle bug in the code for
  transitive-once-calls.
* Had an interesting idea - what if I were to extract a common format from both
  code and assertions, then use more general theorem proving techniques to prove
  properties of the program? Has other benefits like counterexample generation
  etc. Definitely worth looking into in some detail.

## 7/2/2017

* Wrote up a starting note about the model-checking approach.
* Started to do some background reading on model checking for C programs.
* Implemented a non-spinny benchmark for locks - have threads do a bubble sort
  on an array interval while holding a global lock on the array.
* Used the non-spinny benchmark to do some back of the envelope data gathering.
  It seems like the statically analysed and optimised version is indeed reliably
  faster than the instrumented version. Need to verify, but it seems like the
  difference increases with contention as would be expected.

## 8/2/2017

* Collected some actual performance data for the instrumented vs. uninstrumented
  versions of the acquire-release automaton.
* Kicked off a rather long-running data acquisition script that will (hopefully)
  get some good data for the static / non-static versions.
* Reviewed a few of the papers identified as possibly being relevant.
* Fiddled with benchmarks to try to get better data.

## 9/2/2017

* More fiddling with benchmarks to try to get more compelling data.
* Continued with lit review.
* Noted a 500 byte / 20% reduction in binary size when applying static analysis
  to the locks benchmark.
* Begun to work out what I want a model checking tool to be able to achieve.

## 10/2/2017

* Meeting with Robert. Worked out that Jon should be able to log into a CL login
  server, and from there take a look at my failing FreeBSD compilation. Should
  also ask him where I get my hands on the OpenSSL code so that I can look at
  real software that uses TESLA assertions.
* Robert agrees that the model checking approach is viable, especially at the IR
  level. He mentioned that data flow analysis is important - i.e. if several
  data structures are being instrumented, how do we know statically which one
  we're proving properties about?
* Need to think about how best to skip stuff in a TESLA automaton -
  counterintuitively, the best way might actually be to add *extra states* to an
  automaton - this way, we can encode transitions directly past stuff that we
  know to be true. More digging through the TESLA code needed here.
* Did some stats on a promising benchmark - T test seems to indicate that at
  higher levels of contention, there's a significant improvement when using the
  statically analysed version of the code. Difference is less significant when
  there's less contention, but that makes sense as the overhead is all in code
  that only gets triggered on lock contention.
* Have kicked off a larger benchmark.

## 13/2/2017

* Worked on model checking implementation - started to flesh out data structures
  and look at what format the data will be in.

## 14/2/2017

* Big chunk of work on model building - can extract a model from basic blocks
  and functions that shows event flow.
* Worked out a better way of structuring the code for dealign with graph
  construction and began to rewrite the initial prototype using the new
  techniques.
* Written an instruction graph finder for an individual function.

## 16/2/2017

* Big chunk of work on the generic graph implementation - I now have a reliable
  (!) way of transforming instruction graphs depending on how they are required.
* This approach seems way more flexible than what I had the other day, as it can
  extract any relevant structure from a graph if you can express the
  transformation as modification / discarding of nodes.
* The next logical steps are to allow for the graph to increase in size (then
  any arbitrary transformation can take place), to allow for a full module
  instruction graph to be built, and to encode some common transformations as
  helpers (i.e. call graph, call graph with decls etc.).
* Once the graph stuff is done well enough, can look at proving things and
  extracting predicates from TESLA assertions.
* Worked out adding entry / exit nodes to a function graph, as well as releasing
  all of the nodes in a graph. Next step will be to get a range from a function
  graph with these additions, and to splice these ranges into a module graph as
  appropriate. That can then all be wrapped up in a more convienient / less
  low-level wrapper to get useful information out of (if needed).

## 17/2/2017

* Improved the graph stuff quite a lot - almost at the point where I can
  reliably extract an entry / exit graph from a complete module.
* The next thing to improve on is that entry / exit nodes aren't tagged uniquely
  - probably need to extend the instruction graph static constructor to allow
  for a way of tagging them as such? Ahh - we only actually have a single
  instance (every function graph only has a single entry / exit).
* One solution would be to copy function graphs? Problem when doing this is
  recursion depth - possible to generate an infinitely big graph! In fact the
  iterate-until convergence loop I have is the recursion depth control (i.e. how
  many times it runs!). So after a certain depth would just get to call nodes.

## 20/2/2017

* Working on a clone implementation for events so that I can have different
  paths for different instantiations.
* Cloning turning out to be quite tricky...
* Put this stuff on the back burner for now - looking at a call graph, what we
  have is an overly permissive analysis. It still shows that there is a possible
  path (for example) from `exit:lock_release` to `enter:lock_release`, which
  isn't what we want at all.
* Worked out that all the stuff about cloning was stupid - we had a way of
  making copies available for free (rather than caching!)

## 21/2/2017

* Extended the graph implementation so that it can extract assertion site events
  as well as function entry / exit events.
* Looked into the behaviour of TESLA wrt. external functions.
* Wrote up what the model checking algorithm is actually going to have to do.
* Some bug fixes etc. on the graph builder.
* Started to implement the model checking algorithm for the subset of states
  that we care about at the moment.

## 23/2/2017

* Implemented my template-hacky version of generic traversal of the successors
  of a particular node.
* Worked out why assertion site events were failing - for some reason the file
  path my locations have is null terminated so prints equal to the TESLA
  internal ones, but compares unequal.
* The problem is that the strings are read including the null terminator, but
  when written to disk, this obviously get stripped!
* The strategy I've been taking so far doesn't seem to really be too viable
  unfortunately - infinite loops etc. make everything very, very confusing.
* What if I were to implement a *bounded* approach? Property checking on linear
  paths is very simple.
* Would need a way of generating paths of a given length through the graph.

## 24/2/2017

* Built a small tool to extract finite-length traces from an IR module.
* Need to do some thinking about how to treat looping states - one possible way
  is to look at B chi automata together with a way of recognising a trace as
  being an "infinite" loop (ie. if it cycles some subsequence all the way until
  the end). Note: it's OK to have an infinite loop! If we loop infinitely
  without ever violating an assertion, then the program is in a state where it
  loops infinitely!
* More generally, we won't be interested in a lot of traces. We basically only
  want the ones that are a cycle or that traverse a whole range.
* Checking an assertion will be semantically different between these two cases -
  we need every terminating trace to match the assertion, and something else for
  cycles.
* So we want an algorithm that looks at terminating traces and checks them
  against a TESLA automaton.

## 25/2/2017

* Wrote up a bit about how the sequence checker should be working in practice,
  as well as the subtleties involved with getting it right. I think the
  `mult_acq` failures I was noticing are probably to do with the "forgotten
  start" problem, but need to verify that.

## 28/2/2017

* Improved the API for the model checking algorithm.
* Started to pick apart the sequence checking that we need to be doing to detect
  failure events.
* Realised some flaws with the way subautomata are handled.
* Have an initial (but basically useless) implementation of the bounded model
  checking algorithm. Next step is probably to get a more uniform tracing system
  going so that it can be debugged - doing so at the moment is very hard and
  ad-hoc.
* Looked at adding tracing but it got complex very quickly - might have another
  go later.
* Worked out how repetition in a sequence will be handled.

## 1/3/2017

* Realised yesterday evening that the solution to my problems is to separate out
  completeness and correctness in the specification of my checker. The idea is
  to ensure that every event is accounted for by an assertion *and* that the
  trace satisfies the assertion. This abstracts the lookahead issues being
  experienced yesterday.
* Implemented the correct / complete mechanism for checking properties against
  traces. The behaviour is now correct on the simple example programs - `basic`,
  `basic_indirect`, `no_*`, `mult_rel` all work correctly. The more complex
  examples (`mult_acq`, `other`) do not yet, as the current event graph
  structure is not expressive enough to check the properties they depend upon.
* Begun to think about how constraint analysis on LLVM values might be
  implemented for my current setup.
* Wrote a script to time how long it takes to model check various programs - can
  go into the writeup as a demonstration of worst case / best case performance
  when using the tool.

## 2/3/2017

* Jon got back to me regarding build failures for the FreeBSD kernel - he has
  been able to build it successfully so it sounds like I've probably been doing
  something wrong that can be easily fixed.
* Implemented a first stab at a variation of the checker for cyclic traces - the
  idea of the algorithm used here is to allow for the assertion expression not
  to fully match, as long as every event in the trace is checked by some part of
  the assertion (i.e. relax correctness but still enforce completeness). Need to
  formally justify this I think, but it seems to work.
* Worked out that we probably do want to check off events inside the boolean
  expression matcher (at least unless there are some subtle effects I haven't
  thought of yet).
* Got instructions from Jon on how to properly build the TESLA-instrumented
  kernel.
* Lots of thinking about what we're actually able to / want to show using a
  constraint system on return values.

## 3/3/2017

* Done some more thinking about how to improve the interface to the model
  checker. The key point is that we want to recursively generate root checks,
  and have them percolate back upwards *only when successful*. This improves on
  the current approach of using shared state.
* I've also realised that my approach to checking boolean expressions is wrong -
  with the new approach, what we want is:
  * `or`: return the matches from the first expression that matches
  * `and`: all expressions need to match - question of multiple checks?
  * `xor`: not clear here - ask Jon to clarify semantics
* Implementing this means we won't need the tagged trace approach any more, and
  that the return type from a checker will include a map from events to
  expressions. Completeness follows easily from merging these maps.

## 7/3/2017

* Begun to work on improving the model checking algorithm for sequences, though
  it does remain quite complicated.
* Might be worth coming up with a much smaller test harness that I can use to
  work on the algorithm - running it on full program traces is unwieldy. Note
  that this is not exactly trivial (need to build a mock event graph etc.)
* Also noticed that traces can contain duplicates due to the way they are
  generated - for example, we might have the `lock_acquire` event several times
  *all with the same pointer*.
* What this means is that we probably want to copy events before they go into a
  trace, so that all the pointers to events in a trace are distinct (memory
  usage will go up).
* Maybe deduplicating will actually be more complicated than we need. Instead of
  storing event pointers (which are not unique), we store indices of the
  relevant events?
* Key design problem: we may even need to backtrack in some situations!
* The problem is that sequence repetitions are greedy - the initial repetition
  will consume all of the events when it should leave the final one to be
  consumed by the last part of the sequence.
* Had an interesting idea - maybe worth prototyping algorithms in haskell before
  going to the more complicated C++ environment?
* A new algorithmic tack - discard "don't cares" from event sequences, then
  generate sequences of delayed root expressions from the expression being
  matched. Then look for simple root matches between sequences.
* Noted that `BE_And` is actually unimplemented...
* As I had sort of suspected, `BE_Xor` is really just inclusive or - as long as
  one of the expressions matches, then the whole thing will do. Treat it as such
  for now and revisit if we find breaking cases.

## 8/3/2017

* Investigating a library / app I can instrument with TESLA and then show
  improvements when statically checked.
* First investigation is SQLite - notoriously reliable software that's
  distributed in such a way as to make it easy to compile. 
* Got SQLite compiling against TESLA properly, but need to work out if it's the
  right choice for trying to instrument and show the benefit of static analysis
  on.
* Looking at demos, the OpenSSL example from the paper is actually a minimal
  example that verifies API usage rather than instrumenting the OpenSSL code
  itself. Maybe worth looking into things on this model rather than trying to
  instrument a whole library?
* Question is how to come up with an example that focuses on performance while
  also being statically checkable?
* Running example has been locks - what if I were to write an application that
  uses locks for something (and needs them to be verifiably correct). But should
  also think about other kinds of things we might want to look into - for
  example, the TESLA kernel instrumentation defines an assertion that verifies
  we're in the call stack of a particular function.
* What about some kind of server? Think up a task that could be achieved by a
  server and work out some assertions that can go on hot code paths.

## 9/3/2017

* Worked more on implementing basic stuff that will underpin the client / server
  example. Will be at the point soon where a full protocol negotiation can take
  place.
* Design of what we have isn't quite what it could be - both ends speak the same
  protocol, so what we want is a interface that anyone who implements the
  protocol can implement. Then the main protocol implementation for the server
  and the client will be similar (with the difference being that the client
  starts by sending a message and the server by receiving).

## 10/3/2017

* So the current architecture for the embeddable server raises an interesting
  demonstration of why TESLA is useful - control flow is *data dependent*.
* Tinkering with how TESLA behaves on multiple threads - seems that the only
  reliable way to get instrumentation to behave as expected is to have it run
  over the whole thread lifetime (i.e. in the server example, this is
  `write_to_fd`).
* Worked on making control flow much more explicit in the client and server
  implementations.
* Started to add TESLA instrumentation to the examples.
* Think I'm running into the struct-by-value calling convention problems again -
  when trying to instrument the function `send_packet`, the instrumenter blows
  up because there's an argument count mismatch. Should try converting this
  function to take a pointer and see if that makes a difference.

## 11/3/2017

* Investigated the `send_packet` problem - it does seem to be a limitation in
  TESLA when looking at pass-by-value structure types (as I've seen before in
  other places).
* Looking into the FreeBSD TCP implementation - there are lots of reasons why
  instrumenting this code with TESLA isn't likely to be possible in the time I
  have available:
  * Lots of code to understand and instrument correctly
  * Heavy use of macros, long functions
  * Build process as part of the kernel is tricky
* One possible option as a full evaluation would be to build a TUN program
  implementing TCP, but instrumented with TESLA - this would be a decent-sized
  piece of code to write, but would be very suitable for performance analysis as
  it's a "real" protocol.
* Possible problems with this approach are:
  * Implementing TCP is complex - could be a big task just to get running
  * Static analysis might be tricky (but impl. dependent)
* Evidence that this might be a good strategy is from the client-server
  experiment implemented so far - have seen rough performance improvements in
  the uninstrumented version (though the performance characteristics will be
  different because of multithreaded load etc).
* Even if static analysis doesn't turn out to be particularly useful in this
  case, a TESLA-instrumented userspace TCP implementation is still a useful and
  interesting artifact (along with an analysis of why the static analysis
  failed, and what future work could do to resolve that problem).
* Am able to produce microbenchmarks that show the server performs ~5% better
  when not instrumented using TESLA (for 10-50 simultaneous connections) - good
  motivating example for static analysis. Probably better than the spinlock
  example - more 'real work' being done.
* Worth noting the performance characteristics of this example - runtime is
  linear in the number of connections (as would be expected, as all the blocking
  tasks need to be serialised), but the runtime is 5% better regardless.
    
## 12/3/2017

* Fixed up the git repo a bit as some branches with work in progress were a long
  way behind master.
* Have implemented a basic TUN program that can be given an interface and will
  read packets successfully! tshark verifies that it sees TCP SYN packets
  arriving at the interface.

## 13/3/2017

* Meeting with Robert - talked about the possibility of instrumenting a
  user-mode network stack, and he agreed that it is a definite avenue for
  progress & writeup. Recommended I have a look at LWIP as a prominent example
  of an existing user-mode stack.
* Additionally, he reckons that finding a "real" example of something that uses
  locks would be a good idea (e.g. a concurrent sort implementation).
* Also recommended I talk to Alan about the model-checking aspects of the
  project.
* At a first glance, LWIP looks to be less complicated than the FreeBSD
  implementation of TCP/IP - problems with the BSD one include the pluggable
  API etc.
* Idea of LWIP seems to be that by providing hooks, you can attach the
  implementation to whatever device driver you want.

## 14/3/2017

* Worked on getting LWIP built with TESLA - eventually have managed to get it to
  a point where the core LWIP static library can be built with TESLA enabled.
  Next step is to build a small example application that uses LWIP and make sure
  it will all run etc.

## 15/3/2017

* Running into mysterious compilation problems so have taken a step back and got
  the unix suite up and running without TESLA - have verified that I can send
  and receive TCP packets to an LWIP application.
* Can now run a TESLA-instrumented version of the unix simhost application which
  is neat - though to do performance benchmarking of any kind, I think what I
  really need is a custom TCP app (because the simhost shell only allows for
  manual control of sent / received data).
* Used the same process to get a similar setup for a bespoke app.
  TESLA-instrumented simhost remains around as the build process is generic.
* Done some reading into the raw TCP API exposed by lwIP - seems like the way to
  go.
* Seems like I'm doing something gravely wrong in my app - it's causing the
  whole network interface to die somehow so that I can only log in via the
  serial console and need to reboot. My suspicion is that the app is binding to
  *every* IP address, meaning that it kicks my mosh session off.
* Noted that unreliablity in my simple TCP app seemed to be down to the
  implementation being overly simple (as the raw echo app works just fine).
* Next steps for this line of investigation are:
* Work out an appropriate benchmark to use - seems like there are a few echo
  benchmarks floating around online that I can base one on. General idea is to
  fix a packet size, then use a bunch of threads to fire off packets as fast as
  possible.
* Then need to start digging into the internals of lwIP to see where
  instrumentation can be added to the TCP stuff inside, and do comparative
  benchmarking

## 16/3/2017

* Wrote mid-term project report and sent it off to Robert.
* Emailed Mycroft enquiring about a meeting to discuss what I've been working on
  so far.
* Going back to the generative approach to model checking - we want to
  recursively extract all the possible assertion traces from an assertion, with
  the insight that we don't necessarily need to be lazy about it. We have a
  length bound - an assertion trace that's longer than an event trace can't be
  succesful as we're looking for a 1:1 match.
* How do we want to go about doing this? Each type of assertion can have its
  own way to extract a set of possible assertion traces up to a given length:
  * Boolean or: union of all the disjunct cases
  * Sequence: generate set for each sub-assertion up to a length, concat, filter
  * Root exprs: singleton sets
  * Null: empty set
  * Subauto: recurse

## 17/3/2017

* I think my abstraction is slightly wrong on the model generation - sequences
  and booleans can have multiple possibilities, so we need to return a
  collection of generated models rather than just one.
* So we need a way of turning a vector of sets of T into a vector of T by
  choosing.
* Type is: `vector<set<T>> -> set<vector<T>>`.
* Trying to work this algorithm out but it's not going so well initially.

## 18/3/2017

* Got the basic form of the model generation algorithm working. Next step is to
  add support for repeated sequences.
* Can't check directly - need to filter traces for for the events that will
  actually be checked by something in the automaton. Then we can compare traces
  to models.
* Basically we're back to the start now - we want to return a bool depending on
  whether a root expression matches an event. This means we can simplify the
  model-checking API by getting rid of recursion.
* Need to double check my filtering idea for correctness.
* Finished the generative model checking algorithm - at first glance it seems to
  check the existing assertions just as effectively as the previous algorithm,
  and (informally) seems to run a bit faster.
* Possible problem when running on larger programs is that the generation of
  models and traces consumes *a lot* of memory. For now I don't think this is a
  problem, but it's worth noting as a future improvement. There should be some
  decent avenues for improvement as we're very naive about how we generate the
  sets.
* Next thing to think about is the interface by which the static analyser can
  call into the model checker. Currently the command line interface does it, but
  it would probably be best to expose an actual API through a library that the
  CLI tool and the static analyser can call into.
* This API is just the same as what the command line tool does - it can just
  instantiate a model checker and call the SafeUsages method to get the
  information (need to construct an event graph and model checker for each usage
  really).
* The entry point problem is really an issue in the multithreaded context -
  might be able to ignore it safely in a single threaded context (and just
  choose `main` for a suitably high bound).
* Also need to think about how we might go about picking bounds when running the
  model checker (heuristic on the module or the assertions?).
* Need to work out how to generalise to cyclic traces (prefix match?)

## 19/3/2017

* Looking at getting the static analyser hooked up to the model checking
  interface.
* Done lots of boilerplate code to get a new pass integrated into the static
  analysis tool (note: would be nicer to have a base pass that allows subclasses
  to implement `ShouldDelete(Usage *)`, and can then handle the boilerplate of
  copying itself).
* Have successfully managed to get the model checker running from within the
  static analysis tool!
* Implemented checking of cyclic traces (as a prefix matching check).
* Ran the correctness test suite against the new generative model checker - it
  has fixed the sequence bug, but the return constraint extension remains
  important.
* Next step is to try and work out how best to do return value constraints - it
  is still holding back progress on completing the test suite.
* Have done lots of thinking about return value constraints - arrived at what
  seems like a solution involving strongest inferences for a basic block.

## 24/3/2017

* Starting to work on building out the system for handling logical constraints
  on function return values.
* Now that the class hierarchy for conditions is fleshed out, the next step is
  to compute the inference for a given function using the algorithm I worked out
  previously.
* It actually looks like I need to implement simplification and flattening
  before I can implement the inference algorithm - this is because it requires
  inferences to be simplified in order to deal with recursion (I think).
* Note a further opportunity for simplification - splits on branch conditions.
* Simplification could be taken further by moving to DNF / CNF, but this
  involves introducing negation which I'm avoiding for now. Also, we can
  eliminate any top-level const trues when simplifying ANDs.
* Inference can be computed at the function-level, so we need to accept a
  Function as a parameter and return a mapping from basic blocks to conditions.
* The basic algorithm seems to be working fine - can get the results as expected
  on simple graphs. The next step is probably to implement deduplication of
  conditions so that the algorithm can actually iterate - at the moment you end
  up with the same condition ANDed together with itself lots of times.
* Note that this type of analysis loses a bunch of information about the
  ordering of events - from `a=true /\ a=false`, what conclusions can we draw?

## 25/3/2017

* Continuing to look at simplification and normalisation for stringest inference
  terms.
* Trying to find a betetr way of editing remotely - latency is killing me.
* Worked out an rsync based workflow eventually - have also realised that
  recording the CMake invocation I use to build TESLA is a good thing to record
  somewhere.
* Maybe worth noting that I seem to have found a fairly minimal incantation to
  get TESLA to build properly - will be a useful jumping off point in the future
  when I come to improve the documentation and portability.
* Implemented conversion to CNF and some simplification - one last thing to do
  is removing duplicates from ANDs, then equality can follow.

## 26/3/2017

* Added some small things to the simplifier. Found a mysterious bug in which a
  branch changes "polarity" when being simplified, but haven't managed to debug
  it yet.
* Looks like the problem is in the CNF conversion rather than simplification.
* Fixed the frustrating bug - now need to re-examine the logical framwork in
  which I'm examining this analysis (i.e. what AND / OR actually *mean*).
* I think what I refer to as logical OR is really an accidental *ordering* on
  events. As well as this, logical AND gives us a set of *possible* event
  traces that could have happened. A sequence of TTT...F etc or just F.
* The way the "CNF" simplification works, we end up sort of accidentally getting
  length-n possible return sequences.
* So how do we check these against a pairing of calls / return values?
* The first step to work on here is to do backwards inference to map these
  results back onto call return values. Start with the simple case of binary ops
  with constants, then maybe do a speculative writeup of how it might be
  extended.
* Should also do some work to formalise what I've stumbled on here (i.e. write
  it all down more precisely so that it can go into the dissertation).
* Looking at an alternative propagation algorithm (forwards rather than
  backwards) that should make ordering and tracing more explicit.
* How to handle overlapping inferences? Maybe only allow backwards inference
  within a basic block.

## 27/3/2017

* Starting to get the basic algorithm working for forward propagation of
  inferences, but the simplifier needs some work. The ideal representation for
  this kind of thing is obviously a BDD, but that's a lot of implementation
  work. Worth noting as a future (summer?) improvement to this analysis.
* What would a BDD version of these inferences look like? I think it would
  probably require a major rewrite of the whole system unfortunately - probably
  worth saving for future work.
* Maybe looking at doing a Shannon expansion on the resulting inference? Then
  once the variables are pulled out, the resulting expressions are just over
  const true / false and can be simplified really easily.
* Will need to implement mechanisms for substitution, free checking etc.
* Implemented some of this stuff - next step is to fix up checking for const
  expressions so that it actually works, and then to use that to automatically
  simplify expressions when we split on them. Also need to implement
  simplification of AND / OR when they contain opposing branches.
* Worked on improving the simplifier some more. I think the opposing branches
  improvement should knock it over the edge of being useful.

## 28/3/2017

* Made a templated, generic version of the simplification routine so that the
  dual structure between AND / OR isn't left to copy / paste.
* There are still some cases that don't get handled properly by this
  simplification - maybe should look at the problem from a slightly different
  angle. Doing arbitrary simplification will always be hard without completely
  going down the canonicalisation / BDD route.
* Worked out an implication checking algorithm that "answers a question".
* I think the simplification check can be left as-is for now - simpler formulas
  are better for debugging, even if they aren't quite normalised.
* Need to also work out how best to do termination checking on the forwards
  propagation - maybe a change in the implied branches?
* Got the implication check working - can now enumerate the branch conditions
  that are implied at each basic block.
* This allows the iterate-to-convergence algorithm to work properly by comparing
  implied sets. Additionally, simplification is now just an optimisation (as
  without making at least an attempt to simplify at each step, the size of the
  formula being examined becomes unwieldy).
* One thing to look at in the writeup is a nicer formalisation of this stuff -
  can I make it neater than looking at binary boolean operations?
* Interesting question: what about "maybe" conditions that the implication holds
  for sometimes but not always? I think the solution here is to separate the
  sets into "must have" and "could have" (i.e. in all models / in some models).
* Implemented the basic backwards inference to CallInsts. Could be extended in
  the future.

## 29/3/2017

* Begin to integrate inferences with the actual model checking implementation.
  It probably makes sense to move the implication checking and backwards search
  into the inference code so that the interface is a little clearer and we don't
  have to expose the boolean formula stuff externally.
* Starting to think about how to do the return value checking against a trace -
  turning out to be a little bit trickier than I'd thought it would be.
* Lots of working out how to get these pieces to fit together.
* Should refactor the model checker constructor so that it builds the event
  graph itself, rather than having it passed in.
* Finished hacking together the basic block graph implementation and tying it
  into the inference code. Next step is to add a graph search that looks to
  match a sequence of inferences against the basic block graph.

## 30/3/2017

* Finished implementing a basic version of the return value sequence check. It's
  very, very slow but is able to check return value constraints properly
  (finally feature-complete with respect to the hand-coded analyses).
* Now that the model checker works with respect to the test suite, possible next
  steps to take are:
  * Investigate adding TESLA assertions to LWIP / parallel sort
  * Begin to properly formalise the algorithms in the model checker
  * Think of a better way to prove a sequence isn't observable (SMT?)
  * Characterise model-checker performance
  * Try to make the model checker faster
  * Lit review to try and find interesting material
* Worked on fixing the catastrophic exponential performance - using a disproof
  method that aims to find impossible assertion bigrams in the basic block
  graph. I think there is still room for improvement here as there's a lot of
  recomputation.
* Hoisted the bigram computation out of the checking function. This makes things
  a little faster again.
* While I'm at home and the latency isn't good enough for interactive usage of
  `nikola02`, I think a useful next step would be to start to look at formally
  stating what I've done to check the TESLA assertions, and try to poke holes
  in it from a theoretical standpoint. As well as that, a bit of reading to get
  context and terminology sorted out would be a good idea.

## 31/3/2017

* Next step is to do a bit of work on formalising the model checking stuff I've
  done with TESLA.
* Good locking example might be a linked list? Gramoli paper on synchronisation
  and benchmarking could be interesting.
* Might be interesting to include a section on the usability of TESLA (including
  factors like building, integration into a project, compile times etc.)
* Worth clarifying that the traces we generate to be checked are generated by a
  very conservative, path-insensitive approach. The checking algorithm is
  independent of trace generation, so it wouldn't be a fundamental impossibility
  to implement a system that does path analysis to filter traces out that cannot
  happen.
* Starting to get down notes and thoughts on formalising TESLA assertions
  abstractly.
* Doing a bit of LaTeX setup so that I can get this stuff down into an
  approximation of a section.

## 3/4/2017

* Looking to try and implement an FSM library that I can use to improve the
  initial model checking algorithm - we don't need to generate every possible
  accepting sequence, just check whether or not a trace is accepted by the
  machine.
* Implemented a big chunk of the FSM algorithms (NFA -> DFA etc.).

## 4/4/2017

* Worked on integrating the FSM library into TESLA - can now construct a DFA
  with edges labelled by TESLA root assertions. The next step is to check input
  sequences against these machines, and to add a transducer method that allows
  us to output a sequence of constraints as a trace is checked against the
  machine.
* Should look into Buchi automata now that we have a more explicit notion of
  cycles.

## 6/4/2017

* Adding a transducer method to the FSM library that will allow a new sequence
  of values to be generated. This is similar to the accept methods - what it
  will do is accept an input E, and if it's accepted by the edge, apply another
  function to get an output value.
* Meeting with Alan to talk about the project in general terms.
* Have realised that the model generation approach is here to stay, at least for
  now. A finite state machine built using my template library isn't quite a
  drop-in solution, as we don't have a good notion of determinism for events -
  to push a trace through the machine would require us to have a precomputed
  notion of what return values are needed. Interesting for the writeup.
* Found an interesting paper on safety vs. liveness that mentions Buchi
  automata. Could be a useful one for formalising TESLA.
* These automata accept a string by stipulating that the last state loops
  infinitely - this is a bit like what my definition of cyclic traces is like.
* Begun to work on writeup in earnest, looking at the background section on
  TESLA first of all.
* Looking again at LWIP - doesn't look so promising after all. The
  callback-styled API means that it's unlikely I'll find a hot code path that's
  also amenable to useful TESLA instrumentation.
* The other problem is that functions are large, have deep nested branches, and
  don't often call other functions.
* This all adds up to a reasonable case study on "what makes static analysis
  difficult".
* Should look into exporting counterexample traces when model checking fails -
  makes this all a bit better if we can do that.

#7/4/2017

* Thought on instrumenting LWIP - as long as I can get *some* assertions into a
  hot path, it should be enough that I can demonstrate a use case (i.e. that
  they are statically analysable). The actual properties asserted are in a sense
  less important (one or two good examples is probably enough).
* Can then extend the evaluation with a detailed study of what makes it hard to
  actually do the static analysis (with code examples from LWIP).
* More work on the writeup, making a start to the implementation section.
  Chapter titles could probably do with some work.

## 8/4/2017

* Getting through more writing on the implementation section.

## 9/4/2017

* Plan is to keep going on the writing, ideally finishing a first rough draft of
  the implementation chapter.
* Worked on parallelising the parallelisable part of the model checker to see
  how much extra performance that will squeeze out (run into subtle bugs -
  probably not worth carrying on. Should do a full rewrite).
* ModelGenerator is the really bad part with regard to performance - any future
  work would really need to do that refactoring to use the FSM stuff instead.
  Part of the problem is that it consumes so much memory.
* Started lit review and to look at counterexample outputs.

## 10/4/2017

* Finished adding a hacky counterexample output to the model checker - to be
  more strictly useful, the internal printing functions I use would have to be
  improved somewhat.
* My thought that model generation is the limiting factor is wrong - trace
  generation is the killer. A more efficient approach than computing them all up
  front would be a good idea.
* Moved over to checking bounds incrementally - for every length at which it is
  possible to terminate, generate models and traces then.
* The general hypothesis is that it can detect failures really, really fast (but
  you can run it for a long time on successful cases to be sure).
* Checked the incremental adaptation of the algorithm - still correct on the
  acq_rel test suite.
* More opportunities for parallelism arise - why not have a thread pool pick up
  work from a queue (work here being a depth to check). I think that as long as
  the cache is guarded appropriately in the trace generator, everything *should*
  be pretty parallelisable.
* Looking to see how much (if any at larger depths) of an advantage we get from
  parallelising the model checker aggressively.
* Seems to be that it does help quite a lot (although more important are the
  changes to stop early on failure!)
* Next logical step would be some kind of minimisation / smarter search. i.e. if
  we see that the model doesn't leave state x when executing a loop, don't go
  through that loop again?

## 11/4/2017

* Wrote up part of the lit review on bounded model checking.
* Improved some figures in the background section of the writeup.
* Added a performance overhead section to the lock modelling part of the
  implementation writeup.
* More tinkering with LWIP in a way that isn't entirely promising---definitely
  worth a writeup as "why static analysis on real code is *hard*".

## 12/4/2017

* What to look at today? Dissertation is at 6700 words with case study, partial
  lit review, intro and conclusion to come. Things to look at doing just now are
  a look at why it's hard to instrument LWIP, more lit review or trying to find
  a more "security" oriented property that I can assert. Background section
  could maybe also do with a bit of work, but less important.
* MOPS paper gives some potential applications of security automata - should
  look at these and find a similar one that lies on a "hot" code path in some
  tool.
* Why can't TESLA express "the program *does not* do something" - these are
  safety properties after all? So far as I can tell, TESLA is not actually
  capable of expressing these properties.
* Question is what properties can I assert and check, and can I find somewhere
  to put them on hot code paths?
* Things to think about:
  * TESLA changes the build system - hard to integrate into existing projects
  * Assertions are written by the programmer - assertion libraries?
  * Coding style is an obstacle to using TESLA
  * Why not use TESLA+static as a security guidance tool?
* Idea: write a reusable library that attempts to stop programmers from using
  setuid wrong in new projects. Maybe not setuid specifically, but a library of
  checked implementations of things that could easily go wrong when writing C
  programs.
* What kind of things can TESLA assert?
  * Assertions state that on this code path, something holds
  * Bounded previously / eventually.
  * Good for managing resources of some kind - file descriptors, locks etc.
* Typically users are responsible for cleaning up their own resources - what if
  the library contains TESLA assertions that mandate the user does so?
* So this library should be easy to integrate and provide (ideally) zero-cost
  checked abstractions.
* Another TODO is to fix how the model cheker handles arguments (so that it
  actually does)
* The general idea of this seems to be sound - library functions are able to
  describe their own safe behaviour within the TESLA bounds.
* There's an interesting point to be made about abstraction layers within a
  system - TESLA asssertion style allows the programmer to protect
  *well-defined* access points and to protect the use of those. Can then
  restrict usage of "dangerous" functions.
* This style is really defensive coding, and would ideally be supported by
  another pass that prevents the consumer of the library from doing unsafe
  things (e.g. unprotected calls to fopen etc.)
* Fixed a small bug in the model checker that caused it to report a lot of false
  positives when assertion site events weren't reached at all.

#13/4/2017

* Continuing on the idea of a "safe" plugin / library interface that exposes
  operations with associated temporal invariants.
* Found a bug bug in my implementation - function calls across translation unit
  boundaries weren't being preserved. Fixed by adding a safe getCalledFunction
  wrapper.
* Weird issue in libtesla - fixed by linking against a freshly built one. Not
  sure what the problem would have been as I have never modified that code.
* Interesting thought for plugin stuff - it would be possible to distribute a
  bitcode file together with a TESLA manifest as one compiled file (to avoid
  source stuff).
* Put together a really small benchmark for the normal / TESLA / static
  comparison on the KV store - able to show very big performance degradation
  when using TESLA, but static analysed version is as good as nothing at all!
  Protocol is still kind of broken, but the idea still holds water.
* Figured out that the LLVM bitcode wrapper for embedded manifests is actually
  really simple, and the resulting file can be compiled as normal! What would a
  tool to work with these files look like? It would be possible to get the
  library (preinstr) bitcode and distribute it as a single file with a manifest.
  That could then be compiled wih the plugin code to build the whole app.

## 14/4/2017

* Should start to put together the case study section of the evaluation so that
  I can have a full rough draft ready soon.
* Wrote up my findings on "static analysis is hard" with respect to LWIP.
* Started to put together a skeleton of the the section on safe plugin
  interfaces.
* A bit of reading Zobel.
* First draft of introduction.
* Short investigation into how separate compilation could be applied to plugins
  by minimising compilation overhead - ran into an issue with assertion
  locations (not compiling plugin.c with TESLA properly?)

#15/4/2017

* Things to start looking at:
  * Fixing argument handling in the model checker
  * Writing conclusion
  * Thinking up a more realistic library interface for checking
* Managed to get arguments handled properly in the model checker, and added an
  example test case to show that it does work. Hacked a solution to an assertion
  failure that isn't really correct but works for now.
* Interesting idea is weak functions to simulate function pointers at compile
  time---can say `if(func) ...` for a function with the correct interface.
  Library would do this to let the user optionally declare their own hooks, and
  so that the user doesn't have to stub everything!
* Looked through the FreeBSD TESLA assertions a bit - maybe worth noting
  somewhere that these aren't amenable to static analysis for whatever reason
  (isolated, indirection, large code, ...)
* So it is possible to compile a library ahead of time - errors yesterday were
  me being daft. Should make a tool that can inject a manifest into a bitcode
  file wrapper. The wrapped bitcode files can behave just like any other bitcode
  file, but can have their manifest extracted to allow for instrumentation.

## 16/4/2017

* Worked out a reasonably reliable way to compile apps against a precompiled,
  archived library built using TESLA - the app depends on the .m.bc file, then
  compiles / extracts as appropriate.
* Should now start to look at how TESLA behaves in the presence of weak linkage,
  and to build a more reasonable example.
* Idea is that the library obviously shouldn't have to supply `main`, but can
  give interface functions that make safe calls to weak user callbacks.
* Need to fix my argument checking logic - breaking on simple cases from the
  server app testing.
* Fixed the argument checking logic (properly this time!) - constant and
  wildcard arguments are correctly handled.
* Here's an interesting idea - why not interpose *between* LWIP and client code?
  Write a wrapper library that registers callbacks but is able to maintain TESLA
  invariants! This would be the best of both worlds - applicability to real
  code, while still on the idea of safer library interfaces! Why not even modify
  the actual tcpecho_raw example? Good for benchmarking!
* So what we're looking at is a sort of shim layer / reimplementation of the
  tcpecho_raw app? Sort of - it'll verify usage of the API in a more general
  sense. Users of the API supply implementations of the callback functions, and
  the wrapper is responsible for initialisation using those functions only. The
  initialisation function needs to be concrete so that there's a place to hang
  assertions from.
* The API wrapper declares the weak functions, and links with an implementation
  to make them work. I think that to make instrumentation work, we need a
  further layer of indirection:
```
actual_cb_func()
{
  if(user_cb_func) { 
    TESLA_WITHIN(actual_cb_func, ...);
    user_cb_func(); 
  }
}
```
* Then, the initialisation function would register `actual_cb_func` as a
  callback.
* Benchmarking - measure throughput for this version compared w/wo static
  analysis and compare to the actual version?
* Can build the tesla app example against a copied-in version of the raw echo
  application code. Next step is to split off a wrapper, removing app-specific
  code as we go. Need to probably give user hooks into the fixed init / accept
  functions. Have a functioning echo server built using the weak function
  interface - the next step is to build the wrapper implementation against
  TESLA.
* Problem faced is that the virtual interface is *much* slower than making real
  function call - but why should that be so much worse than previously?

## 17/4/2017

* Fixed the mysterious performance issue - the wrapper wasn't registering a
  whole callback! This meant that polling was the only way it could ever be
  sending data.
* Should probably build the core of LWIP as standard (as we're instrumenting the
  user code interface).
* Core now no longer being built against TESLA. The next step is to build the
  wrapper library and app files *with* TESLA.
* The inference checker needs some work - generated conditions are way too
  complex. Fixed this by taking implications inside the simplifier itself (if
  simplification isn't trivial).
* Fixed a small bug in the called / cast function trick.
* Realising that the model generation approach isn't working for optionals - the
  subtlety is in what a null event should be able to check. Should finally get
  around to introducing the FSM code.
* Trying to figure out the subtleties of extracting a sequence of return value
  constraints from an accepting trace through the FSM. Problem is
  nondeterminism, so we need to have a way of getting at all the possible
  accepting traces through the machine.
* The other approach to this is to try to compute RVC *ahead* of time - so that
  we can get determinism back. Not easy.
* Otherwise we're back to a slightly improved model trace generator.

## 18/4/2017

* Continuing to work on fixing up the model checker. Idea late yesterday was
  that we can reuse the bigram checking idea by keeping track of what we saw
  last.
* Lots of improvements etc. to the model checker - fixed the issues with
  optionals from yesterday, and can now run successfully on the assertions
  placed in the LWIP shim interface.
* Doing some benchmarking with an echo server benchmark found online shows
  definite speedups for the statically analysed version when compared to the
  instrumented version.
* Should also build an example that uses the unmodified LWIP raw TCP echo server
  to see how much overhead we introduce with the library interface.
* Promising results! The instrumented version seems to run at ~61% performance
  compared to an unmodified example, while the statically analysed version comes
  in at 87% performance.
* There's a definite overhead to doing things this way, but it could be argued
  that you can eliminate a bit of this by designing the library to support the
  style from the start (so that you don't need to always go through indirect
  calls).
* Running the examples for longer to get more reliable data.
* Started to write up section on safer library interfaces.

## 19/4/2017

* Worked out what I've been doing wrong all this time for caller-context
  instrumentation. Should extend the checker to work with it.
* Finished writing up the applications section and started to finish up the
  background work.
* Writing evaluation section.
* Should also look into optimising instrumentation in the case where we don't
  have any undeleted usages - special mode to bypass instrumentation altogether?
  Not quite that easy, I don't think - needs to do stuff like deleting magic
  function calls.
* Got to a very rough first draft stage.
* Investigating a build of TESLA on Ubuntu, but no luck getting it to boot on
  the server (I think virtualisation won't work as I'm already virtualised).

## 20/4/2017

* Sent off rough first draft to Robert to have a look at - giving it a few days
  sitting time before I look at it again.
* Investigating some engineering improvements to TESLA - first on the list is
  trying to get it built against an up to date LLVM (incrementally).

## 21/4/2017

* LLVM upgrade got to a point where I'd want to be spending more time on it -
  there are places where the instruction builder API is changing and I'm not
  sure where to get types etc. from. For a future effort, maybe worth going
  straight from 3.5 to 4.0?
* Also definitely worth a look at the build system - things like link order for
  LLVM libraries are horrible to resolve.
* Should look at the "no instrumentation" optimisation when every automaton has
  been removed.
* Implemented this optimisation - big decrease in binary size for the echo
  server. Looking at benchmarking it against the other optimisations.
* Big improvements - the overhead comes down dramatically to less than 1%!
  Investigating where the remaining overhead is coming from (probably small
  enough that it's the extra function calls).
* Put the new version through callgrind - shows the overhead is basically down
  to the extra function calls. The only meaningful difference I can see between
  the compiled binaries is the extra functions.
* Definitely worth noting that this is a best case scenario - as soon as you
  have a single automaton left in the binary, things start to get much slower.
* Added this new benchmarking to the report.
* How could the inference algorithm for values be improved? It's really an
  instance of some kind of SMT problem. It's a per-function analysis, where the
  values in the function generate a set of "constraints". Point of SMT is to
  generate counterexamples. What kind of counterexamples are we trying to get?
  We have boolean constraints - take the negated constraint and solve that for a
  counterexample. Each LLVM value is a function - e.g. it says that value %3 is
  equal to XOR of %1 and %2. Then the final branch condition is asserted in the
  negative.
* OK, but how does this work with unrolling / repeated values? We compute the
  inferences for a function. Then from those we use the SMT solver to work back
  up to call values (and that's all - we only use it for the backwards search
  interpretation). Given a set of branch value inferences, we plug them into an
  SMT model of the function. Then, this can give us a possible valuation for
  return values? Should have a look at a more arithmetic-y example to see how
  far this can go (and if my model idea is correct).
* Further exploration: can I turn the entire model checker into this format?

## 22/4/2017

* Continuing to look at SMT methods and how they could be useful for the model
  checker.
* Problem I have is really that the current approach is sort of backwards -
  information is lost in the process of doing the model check (building a
  trace). To do the return value inference properly, traces need to include all
  the control flow information.
* Generate an arbitrary, data-flow insensitive path through a function. Then use
  a local version of the control flow analysis to work out what conditions hold
  at each basic block.
* Because we have a whole trace like this, we can "ssa" it even further - we can
  then make a linear function that is only the basic blocks executed on the
  trace. Add a final sink basic block? Maintain branches in the linearised
  function. The idea is that there's then a definite sequence of boolean values
  observed on the trace in order for it to reach the end - then we have a linear
  model of a function, and so we can use SMT techniques to build a model!
* So what do we actually need to be doing?
  * Create a dummy function for each trace - same type as original
  * Build a value map for function arguments (and other values?)
  * Add a sink block to each trace that handles untaken branches
* Implemented this function trace mechanism - instead of treating traces as just
  sequences of events, we generate a non-executable function that preserves
  structure *and* encodes the necessary data flow in a form we can use for SMT.
* Things to do now:
  * Extract event sequences from these traces so that we can do model checking
  * Turn the SSA instruction form into an SMT problem by running inference
* The inference algorithm can be simplified now - just check each basic block
  for a conditional branch and pick the correct value (i.e. not leading to sink).

#23/4/2017

* More investigation into SMT solving techniques for the model checker.
* Maybe better to structure the SMT generator as a function pass rather than as
  ad-hoc code - means that I can swap it out more easily for one that uses an
  SMT API along with other benefits.
* Have changed the approach, and can now generate models for boolean-only
  traces - correct condititions are generated on the lock examples.
* Currently what we're looking at is the conditions for a trace to terminate -
  we are only actually interested in traces that include an assertion site. For
  that set of traces, we want to check whether each one is accepted by the
  automaton. We do that by generating the sequence of return constraints for the
  trace, then walking it instruction-by-instruction.
* An instruction matches an edge with a return constraint only if the top of the
  constraint list built previously is satisfied.
* Problem remains with *inlining* - this method does not cope with function
  calls. Need to perform an an aggressive inlining pass to a certain depth?
* We need to preserve function entry and exit events when inlining - add calls
  to a new dummy function at the start and end of each function with a constant
  string argument? Then, do aggressive inlining of the bounding function up do a
  specified depth.
* New problem is that we might not know where a function return value is going
  after the call has been inlined. But we don't care about the actual value -
  add a dummy of the appropriate type after each CallInst that acts as a
  non-inlinable placeholder.
* So: placeholder functions for function entry and return, holding the same
  arguments and replacing the return values of the functions.
* Implemented the stubbing and inlining mechanism - the results of calls are now
  preserved in the unresolved functions, while the function bodies themselves
  are now embedded in the overall function.
* Next step is to fix up the visitor for loads and comparisons. Loads can be
  handled like function calls - we have no idea what will come out of memory so
  it's just an uninterpreted function. Comparisons give us defined functions in
  the same way as a binary operation does.
* Should also begin to integrate CVC4 into the checker rather than generating
  text - the logic will be broadly similar, but internal rather than external.

## 24/4/2017

* Looks like the basic-block based trace generation method is far slower than
  the previous one (but we should be able to do similar things to avoid up-front
  generation).
* Could also apply a CFG simplification pass of some kind?
* Also filter generated traces somehow looking for assertion site events.
* Added support for loads and comparisons - can now generate models and solve
  them for the lock examples successfully. Next step is to integrate CVC4 as an
  internal solver and work on integration.
* Using Z3 instead of CVC4 for now as CVC's buuld system wouldn't play nicely.
* Also worth noting that I'm currently using a theory of booleans and integers -
  a more accurate model of LLVM IR would be to use bitvectors. Maybe worth doing
  that for the Z3-integrated version. The whole question of width is then
  removed, and we don't have to worry about separating conceptual bools and
  ints.
* Can now call into Z3 and get back a set of constraints on function return
  values automatically.
* The next step is to actually check traces against an automaton. Each trace is
  a linear sequence of instructions, disregarding the sink block. We want to
  walk this linear sequence, checking against the automata as we go. The
  nondeterminism with multiple return values is handled nicely by the Z3 solver
  (we can just look the exact call instance up to find the correct path /
  failure).
* Steps:
  * Accept bitcode and manifest as input
  * Generate FSM from the manifest
  * Traverse the FSM, checking whether or not instructions are accepted
  * When a return instruction is reached, accept / reject
* How do we decide when to ignore instructions? Can get a set of edge labels
  from the FSM. Then get function names from those.
* Check the trace for an assertion site first, then do the checking!
* Either decide to disallow Z3 configuration (make it mandatory), or have a way
  of disabling it in the static analysis tool.
* Starting to build the Z3-based model checking tool for integration with the
  current setup.

## 25/4/2017

* Continuing to integrate the Z3-based solver into a new version of the model
  checker.
* Think the checker needs a good bit of refactoring work done to it - it's
  piling up code in a bad way. Lots of duplicate parameters being passed around.
* I think that we want to run the inlining passes before instantiating a
  checker. Then, each usage check can safely instantiate a separate checker.
  This means that shared state such as constraints, fsm etc. can be class
  properties rather than being passed around all the time.
* Each checker instance shares a module, but we can have a static mutex as
  before to guard the argument lookup. Or do we? Each trace function is in its
  own module, so isn't shared. Each depth can create its own trace functions and
  modify them to get arguments safely - more parallelism.
* Problem is that inlining depends on the bound function - how to deal with
  this?
* Can make the checker's data dependencies smaller still by having it take an
  inlined function directly - doesn't need the module then.
* Working on improving the interface of the new model checker, but hampered by
  WiFi blocking the VPN for now.
* Looking into writing up a section in the dissertation about the new SMT-based
  approach.

## 26/4/2017

* Continuing to work on the new model checker integration.
* Finished implementing the Z3 checker - for now it seems slower than the
  hand-coded version, but has the more appealing underlying formalism (and
  soundness for RVC, I think).
* BMC uses the idea of a diameter - then claims that if the property holds for
  an example of length k >= d, then it does so for all examples.
* Z3 itself is obviously very fast - the problem is in the exponential number of
  examples I'm throwing at it. Generating too many basic block splits is the
  crux of it. The problem is that we need the full CFG to use Z3 for RVC, but
  there is a horribly exponential number of traces over said CFG in the presence
  of loops.
* Why not try to encode the whole system as an SMT problem? We can define state
  structures and a reachability relationship on them.
* States are program events - encode as abstract types with reachability?
* Looking into this - can encode an FSM by a 1-step transition relation that is
  then extended by transitivity. Use appropriately-sized bitvectors and enforce
  uniqueness.
* This gives us a model of the TESLA automaton - next logical step is to build a
  set of program events, then describe when a sequence of events is accepted by
  the automaton. Program-side mapping of events onto constants?
* Might be the case that this translation turns out to be a useful summer
  project to work on rather than making it into the dissertation.
* Seems like Z3 struggles with recursive definitions / looping models.
* Solution is to use the inbuilt way of defining recursive functions - how to do
  using the C API?

## 27/4/2017

* Finally internalised what the Z3 API documentation means by macro expansions -
  the client code can unwind a recursive definition for as many iterations as it
  needs.
* Problem remains with the implementation - we are just generating far too many
  traces. The crux of this is looping - a straightforward tree-style CFG isn't
  such a big problem, but the repeated loop is.
* Can we take inspiration from BMC techniques to identify and deal better with
  loops?
* What is a loop?
  * Any basic block with a back edge to a node that dominates it
  * In `basic`, `%1` dominates `%4`, but there's a back edge
* What can we do with loop structure?
  * Unroll as we do currently, but this is exponential
  * Identify what kinds of assertions can be accepted by a loop?
* What can we do with a dominance tree? Lets us identify a minimal sequence of
  basic blocks to exit (in `basic`, this is entry -> 1 -> 5 -> exit). So if we
  check the dominant path to each exit (and fail), what do we know? Not
  necessarily anything, because a loop could have made the conditions true.
* There are algorithms to identify loops in complex control flow structures,
  probably exist in LLVM as well.
* What about computing loop closure? For each loop identified, work out where it
  can get to from each state after n iterations.
* This is probably all future work - big efficiency gains will be possible if
  some of this exponential structure can be removed.
* And it can - solution was annoyingly simple. The CFG I was analysing had a lot
  of branching in, and it turns out that lots of it could be fixed by the CFG
  simplification pass in LLVM.
* Now can look into parallelising the new checker. Each thread owns its own Z3
  context, so can be run safely in parallel I think. The only part that needs to
  be locked is the argument lookup again.
* Interesting to note that the new version consumes very little memory compared
  to the old one, presumably as a result of using LLVM internally for
  everything and being smarter about object lifetimes etc.
* Current performance status: basic at 500 depth in 5 minutes.
* So parallelising the new version isn't as easy as the old one because of how
  pervasively it uses LLVM internals. Might still be possible, but seemed to
  mean littering the code with locks (and still getting intermittent segfaults).

## 28/4/2017

* Meeting with Robert to discuss remaining directions for the project.
* Key points were:
  * Good evaluation needs to focus on *explaining* performance effects
  * Counterexample generation is useful to have
  * Might be interesting to look into "how much modification" is needed
* Next steps: use hwpmc to observe performance effects when using TESLA
  (architectural and microarchitectural). Improve counterexample generation so
  that failing traces can be better understood.
* Counterexample generation - what information would we like to extract?
  * Failure reason
  * Call stack leading to failure
* When do we know about failure? Trace checker `is_safe` method tells us this.
  Unambiguous at the point of calling `next_state`, so we know which program
  event failed to match which automaton.
* Do we want to switch from using bools to some kind of counterexample
  structure? Want a CLI argument to print counterexamples I think.
* Implemented a basic form of counterexample printing that tells the user why
  their assertion failed, along with a callback to the offending event. Useful
  addition to this would be to pinpoint the possible events that would be
  recognised, and to print the FSM. Works as is for now though.
* Now investigating how to use the `hwpmc` tools for detailed profiling of
  programs.
* Starting to tinker with `pmcstat` to get a feel for how I can get explanations
  for observable performance effects.
* Benchmarking ideas need to change a little bit - the timed approach is good
  for throughput, but doesn't work so well when looking at things that pmcstat
  can measure. In this case better to use large fixed-size files.
* Been able to measure a few simple metrics using `pmcstat` against the server
  implementations. The next step is to work out *what* metrics to use in order
  to explain why we see performance penalties for using TESLA - need to work out
  methodology etc in order to get a good writeup out of it.
* Architectural / microarchitectural effects are important - what metrics get
  worse running under TESLA? Which ones make sense to measure in a benchmark
  context?

## 29/4/2017

* Looking into gathering performance metrics for the echo server implementations
  to see where performance degradation is coming from.
* Things to explore:
  * Time taken to echo data into `/dev/null`
* Observations:
  * Bulk transfer rate plateaus for both the static and instrumented versions
  * Through the scaling up and plateau components, relative throughput constant
* So throughput can be measured by sending large files through the server - how
  does running with performance probes enabled change this?
* Quick back-of the envelope test to see how running with the instructions
  counter looks. There does seem to be some effect on relative performance - the
  instrumented server gets 63.3% average under no pmcstat, but drops to 60.7%
  under pmcstat. Relative throughput does remain constant over file sizes,
  however.
* Even with very large files, the maximum throughput demonstrated by each server
  seems to remain constant - same as existing benchmarks, suggests peak
  performance. No degradation from continuous usage / lots of data, or from
  multiple connections.
* Next step is to pick the counters to actually measure.
  * Cache misses (data / instruction?)
  * Branch mispredictions
* Also need to work out what the best way of actually gathering data is.
* Can use `pmcstudy -L` to get the actual name of things that didn't want to run
  before.
* First possible performance indicator - instrumented version is doing a lot
  more memory accesses than the statically analysed version. Need to look at
  cache behaviour as well.
* Consistent across cache levels - the instrumented version performs roughly
  twice as many loads as the static version.
* More stores as well, but not quite 2x as is the case for loads. Generally,
  lots more memory operations for the instrumented version.
* More culprits - branch misprediction is much higher in the instrumented
  version. Primary source of this is inside TESLA library code.
* Should also be doing these comparisons with the unmodified version and the
  halfway-analysed version.
* Obvious increase in number of instructions executed - the instrumented version
  retires newarly twice as many when receiving a 100M file. Same places are the
  cause of this as callgrind indicated earlier - strcmp inside libtesla for both
  memory and CPU cycles.
* Binaries for the static version actually turn out to be smaller than those of
  the unmodified version - suspect this is due to the differences in build
  process (i.e. being able to do whole program optimisation on them).
* So we have data showing slowdown consistently over the 4 versions of the
  server as expected. Now let's run them under `pmcstat` with counters enabled
  so that we can see how benchmark performance changes.
* Counters to be used:
  * No of loads (total, L1, L2, LL)
  * No of stores
  * No of instructions retired
* First step is to get a picture of how enabling these counters will change
  performance. Second step is to actually gather data and see where performance
  is hit.
* Enabling these counters doesn't seem to have a major influence on performance.
  No consistent swing up or down for relative performance, all differences
  within 2%. Seems to be the case that we can run these probes without too much
  impact on performance.
* How best to gather data? Need to get info per file sent, so can't automate as
  easily.
* Memory behaviour seems consistent across versions - hits go proportionately to
  the same places, but there's just a lot more of them in the static / instr
  versions. Across data size as well as versions.
* Instrumented version does ~240% the loads the unmodified version does, which
  in fact holds for instructions in general.
* In terms of the number of instructions executed, the static and instr versions
  are very similar over the portions they share - both do about the same amount
  of work in sunrise / sunset. The difference comes from the instrumented
  version doing twice as much again on updates.
* Haven't yet diagnosed any behaviour in which the instrumented version is
  "worse" than the default version except in the *amount* of work it ends up
  doing - should try again tomorrow with different metrics (more instruction
  related? pipeline stalls?), as the memory profiles are very similar.
* A first look at whether the sunrise / sunset costs could be removed isn't
  promising - because lifetimes are shared between automata, there's no easy way
  to tell whether they can be omitted at instrumentation time (as opposed to the
  actual instrumentation code, which is tied to a usage).

##30/4/2017

* Next steps with profiling - look at instruction / execution related metrics.
  Yesterday's work showed that the memory behaviour is very similar once
  instrumented (more of it, but same proportion of loads, hits at each
  cache level).
* Interesting note that the instrumented version exhibits noticeably better CPI
  than the default versions - hypothesis for this is that `strncmp` is well
  suited to being executed on a super-scalar processor, and so a large
  proportion of the execution time gets good CPI as a result. Some very quick,
  imprecise checking seems to indicate that this is indeed the case.
* Not even the instrumented version makes top-level cache misses - the internal
  LWIP library is well optimised.
* Conclusion from this performance work basically seems to be that the
  instrumented versions don't perform proportionally any worse than the
  uninstrumented versions, but they need to do a lot more work. The bottleneck
  is in `libtesla` comparing lifetimes.
* Firmer conclusions: can see the exact same overhead at sunrise and sunset
  between the static and fully-instrumented versions. The difference is in
  automaton update code - this overhead is then proportional to the number of
  TESLA events triggered on an execution, while the sunrise and sunset overhead
  is proportional to the number of lifetime bounds encountered.
* Because the overhead is primarily in `strncmp` calls, the instrumented
  versions are actually a bit more efficient - they get a better CPI ratio by
  spending so much time doing easily super-scalar operations, and don't behave
  any worse with respect to memory.
* For the most part, the amount of work done doing "tcp things" is about the
  same across versions.
* Result is a clear attribution of overhead to TESLA work being done.
* Added a section to the echo server benchmarks that analyses performance using
  hwpmc.

## 1/5/2017

* Edited implementation chapter, rewriting things where they needed to be.
* Made some minor formatting changes to ensure that the report meets
  regulations.
* Thinking about how to improve counterexample generation - need to make some
  changes to code structure in order to capture and dump all the relevant
  information. If a check result gets a trace checker at initialisation (and can
  access all the checker's fields), then what can we do?
  * Don't need to capture trace, nor do wacky cloning things
  * Still need event and state for failures
  * Can access FSM for printing, don't need call stack param
  * Make call stack a member function to access checker

##2/5/2017

* Made some big improvements to counterexample printing - it can now give
  information that's actually useful about why assertions fail by printing the
  FSM, call stack, current state and failure reason.
* Improved some diagrams throughout the report and added a counterexample output
  as a demonstration.
* Minor changes to wording etc. in a few places.
* What major improvements to the implementation are possible
* Formulating programs as state machines and TESLA assertions as logical
  properties over these state machines. Can program events be seen as logical
  single-state predicates? Each program state has 0-1 predicates true of it.
  Inline and split basic blocks so each has only a single predicate.
* How do TESLA assertions get translated as logical statements on this kind of
  structure? How can return values be linked into this? Splitting - for each
  constraint on a function's return value, generate a distinct state?
* This is conceptually interesting - treat events as things that are true in a
  state rather than things that happen. Use constant propagation and control
  flow simplification to enforce return value constraints (for each possible
  constraint mentioned in the assertion, generate a new state).
* Doing this transformation within the bounding interval would then give a sort
  of Kripke structure that could be checked against an LTLish translation of
  TESLA assertions.
* Investigating the TESLA test suite to see if I can get it to stop breaking so
  horribly. Fixed by working out how to actually use lit python packages.
* Would be good to actually have tests for the static analysis stuff to make
  sure it doesn't break - maybe a summer project.
* TESLA experiments probably need to be extracted out - they make the build a
  bit fiddly.
* Also the Z3 issue complicates things - need to just make it mandatory I think
  as it's the better of the two tools.
* Looking into cleaning up the build - LLVM_DIR is what's needed by LLVM cmake
  configuration. Depends on where LLVM is installed. Then also need the
  appropriately versioned tools.

## 3/5/2017

* Further work on cleaning up my work on TESLA static analysis - need to make
  things a little bit more "production".
* Made some big cleanups that make the static analysis work dramatically easier
  to understand and work with.
* Looked at getting documentation built properly - also need to add
  documentation for my own contributions to TESLA.
* I can't update the ucampas styled pages myself, so the best I can do on my own
  code for documentation is just to add comments and clean things up as best I
  can.
* Re-investigating a build of TESLA on OS X - requires building LLVM from source
  which is a bit of a pain, but there's no reason why it shouldn't work in
  theory.
* Doxygen doesn't seem to want to actually generate any documentation for
  anything - investigating.
* Finally managed to get doxygen properly generating documentation that could
  actually be useful on a TESLA site.

## 4/5/2017

* More work on making TESLA nicer to use (building, cleanup etc.)
* Managed to get a successful TESLA build on OS X for the first time! Worked
  with a default LLVM 3.4 source build, and with protobuf / z3 installed as
  default.
* Should fix the absolute path bug / inconvenience between the instrumenter and
  the analyser. Other than that, the version of TESLA built seems to work on OS
  X well enough.
* Interesting problem - clang tool needs to be prodded in the direction of the
  system headers for its clang version to get it to actually find them.
* Problem is really caused by the TESLA analyser not being in the correct place
  *relative* to where clang34 is installed.
* Solution is to mandate that TESLA and LLVM are installed in the exact same
  place.

##5/5/2017

* Yesterday saw some strangeness because of my pointer type overload - should
  try refactoring that method to return a string rather than writing to a
  stream.
* Fixed that and some other small changes (submodules, warnings etc.).
* TESLA now builds much more cleanly on OS X, as does the separated suite of
  examples. Should see if they work on FreeBSD as well by setting paths etc.
* It works correctly by overriding the correct Makefile variables - no
  modification to the make script required.
* Meeting with Robert to discuss dissertation writing - key points were:
  * Stronger introduction - results up front
  * Could edit related work down a little if word count is needed
  * Mention correctness / FN etc in evaluation
  * Refactor performance etc. into evaluation a bit
  * More terminology - bounded intervals
  * Motivating example of an assertion at the start of background chapter
  * Waiting for full detailed comments from Robert via email
* Truncated model checking paper deals with the idea of extending LTL semantics
  to non-maximal paths.
* Lots of interesting ideas here - TESLA model checker touches on it a little,
  but because of the properties we're asserting, we just want to be able to
  reach an accepting state somehow.
* What I do is exactly their "weak view" - nothing has yet gone wrong! Great
  justification for my choice in the model checker. Worth noting that we have a
  decidable distinction between terminating and truncated paths, so we can
  switch between the weak and the strong views as appropriate (i.e. when the
  path terminates vs. when it doesn't).

## 6/5/2017

* Yesterday looked into some refactorings of the dissertation as suggested by
  Robert. Today continuing with that.
* Examining model checker performance. Preliminary results seem to indicate that
  it's O(n^3) on `basic` - though it could also be that there's an exponential
  factor in the number of conditional branches taken?
* Means that for a *fixed program*, the complexity is polynomial in the length
  of the trace, but exponential in the complexity of the program itself.
* Did some major refactoring of the dissertation, moving echo performance
  analysis into the evaluation section.
* Wrote up an initial version of the abstract.
* Investigating a more proper move to LLVM 3.5 to get the upgrade ball rolling -
  having the test suite means I'm less likely to break things when API methods
  change.

## 7/5/2017

* Pushing on with a proper upgrade to LLVM 3.5 - still some refactors left from
  Robert's comments but not urgent.
* Getting close to an upgrade as far as 3.9 - this is for more up to date than
  before, so is a big improvement. Some weird stuff with lit going on, but for
  the most part it all seems to work OK.

## 8/5/1017

* Trying to finish the LLVM 3.9 upgrade properly.
* Got the upgrade to 4.0 working - TESLA now up to date with stable LLVM!
* Works using a preinstalled (i.e. non-source build) of LLVM 4.0 on OS X as well
  now.
* Added a simpler motivating example to the background section.
* General reading to make sure everything basically makes sense before sending
  back to Robert for specific comments.
* Doing a bit of investigation into how best to set up a project website for
  TESLA (better examples etc). Jekyll theme on github pages is probably the most
  sensible for now I think.
* Should see if TESLA works if dependencies are installed via Homebrew.

## 9/5/2017

* Worked on getting a website for TESLA up and running.
* Need to investigate TESLA's internal generation of inclusive / exclusive or
  automata, and see if my FSM generation code does the right thing when building
  my automata. If not, it might be necessary to actually construct a cross
  product automaton (and write it up etc.)
  * Basically need to see what happens if I write `foo() || bar()`.

## 10/5/2017

* Identified a bug in the model checker last thing yesterday - need to work out
  what's causing it and how I can fix it. Identified problem was that the
  assertion site event in a TESLA demo wasn't being picked up, causing the
  analysis to fail.
* Real problem was to do with an unfortunate interaction between mem2reg and
  argument names that is now fixed.
* Important point - tests only run correctly on Debug build mode, presumably
  because of NDEBUG being set and preventing some examples from producing any
  actual output.
* Conclusion is that mem2reg makes the checking *dramatically* faster, but can
  break variable names. Shouldn't ever make it incorrect, though - it will only
  discount correct things because their name can't be found. Leave disabled for
  now.
* Also being reminded of the existing issues with the parser - can't use by
  value structs at all. Need to fix this in the summer probably.
* Long term, I think the moral is that argument lookup code needs a rework
  (possibly to use the new debug information available about names etc). Tied
  into fixing bugs in the parsing code if at all possible.
* Parser being clang is nice because it makes things "native", but runs into
  issues as described previously. Other implementation options?
* Found an unsafety bug - `possibly_checked` logic needs to include assertion
  sites contained in the automaton as well as entry and exit.
* Repeat [m..n] is broken for my automaton construction - investigate. Fixed -
  cause was just an off-by-one error.
* Change to the alloca finding logic doesn't seem to have broken any tests, but
  it's still something to keep an eye on in case of changes in the future.
* Maybe investigate Sphinx as being a better choice than Jekyll?

## 11/5/2017

* Set up an MkDocs site for TESLA.
* Interesting issue with inlining / argument lookup interaction - because
  assertion sites are "lifted" into the inlined context, if the assertion names
  a variable, it can "escape" its scope and end up referring to something
  different. This seems to be OK in the dynamic instrumentation because there's
  no inlining (and so names mean what you think they mean). The static version
  probably needs some kind of binding / name mapping for inlining - the dynamic
  lookup code isn't really good enough.
* Key point is that name lookup needs to be preserved properly across inlining,
  which means building a mapping of names during the inlining process.
* How can we support this? By inlining, we lose all sense of variable scoping -
  which is how TESLA implements argument lookup. We'd need to implement some
  kind of a hierarchical mapping from values to names that can be queried at the
  time of inlining.
* Hacky solution is to write assertions that use variable names only at the
  topmost possible level.
* Added some more documentation on a simple TESLA example, as well as
  placeholders for a macro reference document.
* Working on a homebrew formula to make TESLA installation easier.
* Formula seems to work - installs on my machine just fine, and trying on Alex's
  for completeness. Easiest installation of TESLA for end users *ever*.

## 12/5/2017

* Worked on the TESLA Homebrew formula some more - it now supports bottled
  distributions, and can be installed from a specific version.
* Could do a FreeBSD port as well, but this does seem more complicated (and
  maybe better to leave until summer or for someone more familiar with FreeBSD
  to do).
* The test suite can now run on OS X as well (as long as FileCheck and lit are
  installed).
* Fixed some small problems with the test suite caused by incompatibilities in
  CPP flags.
* Did some digging into the absolute paths issue - turns out to be that clang
  tools get given an absolute path every time they're run (instead of exactly
  what's specified on the command line). The upshot of this is that the first
  compilation does need to give an absolute path to clang so that `__FILE__`
  gets expanded correctly.
* Working on TESLA command documentation.

## 13/5/2017

* Things to look at today - command documentation needs finishing off, but other
  than that not a lot. Will take tomorrow to look at the dissertation writing
  again (maybe with some reading of Zobel, but today can be a partial break from
  TESLA).
* Maybe replace mentions of "marker functions" with "instrumentation hooks" in
  the writeup? Possibly clearer.

## 14/5/2017

* Got a reply from Alan regarding my first draft - some good points but also
  some that have been fixed already based on feedback frOm Robert.
* Should work in a reference to soundiness somewhere - good justification for
  why it's OK to do finite bounded model checking. Doing this requires that I
  call out exactly why my method is unsound - basically because you can go "off
  the end".
* Fixed up a number of issues with the bibliography and sent the newer version
  of the dissertation back to Alan with some more comments.

## 15/5/2017

* Doing some reading and annotation of Zobel / dissertation to see if I cna
  improve anything.
* Problem with Or / Xor diagnosed - not a soundness problem, as I won't end up
  accepting invalid programs (only rejecting valid ones). Should still fix by
  implementing a cross-product like composition of automata for inclusive or -
  the original paper has the exact construction, so I should be able to just
  translate it and write the extension up in the dissertation.
* Would also be nice to clean up, document and test the FSM code as much as
  possible.
* Also check up on argument-dependent "possibility" checking - what are the
  exact rules for when this works?
* Possible that the SSA-name based method is too fragile and it would be best in
  the long run to just switch to a DI method now that we have a newer version of
  LLVM.
* Completed a pretty thorough review of the dissertation, producing a lot of
  annotations to be worked on before my next meeting with Robert.
* Separating the FSM library out into a "modern CMake" installable thing for
  now, as the compile times are otherwise a limiting factor. Also means that
  writing unit tests is easier.
* Set up CI and a better build structure for the FSM library - next step is to
  add a cross product construction to it.

## 16/5/2017

* Worked on testing and documentation for the FSM library - got Doxygen running
  on it successfully.
* Started to write the cross product construction.

## 17/5/2017

* More work on the cross product construction.
* Got inclusive OR implemented (and noticed more weirdness in the constructed
  automata).
* Lots of work on debug printing of FSMs - helpful especially when they start to
  get large.

## 18/5/2017

* Worked on demo app that uses the FSM library.
* Fixed up the small modifications to my dissertation and sent it to Robert.

## 19-21/5/2017

* Days off

## 22/5/2017

* Meeting with Robert to discuss the latest draft of my dissertation.

## 23/5/2017

* Got the first part of my dissertation back from Robert with detailed comments.
  Working on fixing these.
* 

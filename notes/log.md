# Part III Project Work Log & Notes

## 24/11/16

* Build TESLA on both remote and local machines - it would be good to be able to
  work on both (if ssh is slow or compile times are a problem).
  * Local - ran into a problem when building. Possibly because we require msan
    for something? Can't get it to build for now - try again later if really
    necessary.
  * Remote - seems to build *a lot* faster than on my laptop. Needs libprotobuf
    installed. Possibly better to wait for cluster access rather than mucking
    around in my home directory to try and install it?

* Cluster login - have been added to the appropriate gorups but need to find out
  how to actually log in / install software / etc.

* Reading code and understanding the structure of TESLA
  * Analyzer - tool for walking over TESLA assertions in a program and
    serializing them to protobuf
    * Main entry point in analyzer/tool.cpp parses command line options etc, then
      builds a clang tool and calls out to it
      * Clang tool takes a compilation database and a list of source paths to work
        on
      * Uses a TeslaActionFactory as an argument to the tool - this builds a
        TeslaAction that runs over the source files. Serializes automata usages to
        a protobuf file / string
    * Vistor class walks over automata descriptions and tries to parse them
  * Common - functionality used by multiple parts of TESLA
    * Class that describes the internal automaton / states / transitions between
    * Protocol buffer specification
    * Automatic name generation for internal components
    * Manifest describes the TESLA instrumentation to perform - can be loaded
      from a file (is this the same format that's dumped out by the analysis
      phase / catted together?)
    * Convenience stuff for debugging
  * Instrumenter -  does the work of adding stuff to compiled IR based on the
    analysis phase
    * Adds to the IR based on the actual TESLA assertions that are made
    * Field reference, function calls, struct assignment, assertions etc are
      defined in this part of the code base
    * Translation from actual program events into automata descriptions
  * Tools - the TESLA binary tools that make up the workflow as described in the
    project documentation
    * `cat` - reads all the automata from the filenames passed in, then combines
      them together into a single manifest (doing error checking - for example,
      two automata with the same name but different contents is an error).
    * `print` - just prints out an automata description to the console in a
      bunch of different formats
    * `get-triple` - just a call to the LLVM function that gets the system
      triple.
  * Tests - for automata, instrumentation, integration, parsing, regression
    * Parsing tests put TESLA assertions into a bunch of different C programs
      and use FileCheck to ensure that the assertion grammar is parsed out
      correctly
    * Integration tests build full example programs and ensure that TESLA
      behaves correctly
    * Regression tests cover examples generated from previously found bugs in
      the system
    * Automata covers working on automata protobufs (either written out
      explicitly or generated from C programs) - e.g. that they can be
      concatenated together & resolved properly etc.
    * Instrumentation tests the interaction of the system with LLVM bitcode
      (i.e. that the correct checks are being added to the bitcode files)

* TESLA workflow summary / idea of how data flows through the system
  * Starts from ordinary C programs with no assertions in them
  * Then they are instrumented by adding TESLA assertions to check some
    property of system behaviour
  * A C file with TESLA instrumentation is then *analysed* by the first part of
    the TESLA workflow - the analysis phase produces a `.tesla` file which
    contains an automaton description (these descriptions contain essentially an
    AST of the assertions contained in the program - examples are in the tests
    directory as FileCheck lines).
  * The C file is also compiled as normal into a .ll file using clang once it
    has been analysed.
  * All the .tesla files generated by the analysis phase are combined together
    using the `cat` utility into one manifest.
  * Then, the single combined manifest is used to instrument the compiled LLVM
    bitcode files (giving a set of augmented LLVM bitcode files).
  * The instrumented files can then be compiled using `llc` into a binary as
    normal, with the TESLA functionality included.

## 2/12/2016

* Got set up on the development server by Khilan - managed to get TESLA building
  and installing into a home directory appropriately.
* Manually worked through compiling a program to use TESLA etc.
* Built a somewhat hacky CMakeLists that lets me compile a _simply-structured_ C
  application into a version that is instrumented with TESLA. At some point will
  need to look into how this can actually be done in practice for a larger code
  base! (Ask Jon Anderson?)
* Begun to look into building some very simple TESLA assertions on a small
  program.

## 3/12/2016

* Continued to work on very simple examples learning to use TESLA. Found that
  the documentation is out of date in a number of places:
  * `TESLA_NOW` is replaced by `TESLA_ASSERTION_SITE`
  * `called` seems to have been replaced by `call` but I'm not sure if this is a
    drop-in replacement.
* Notes on using TESLA:
  * `TESLA_ASSERTION_SITE` refers to the point at which the assertion is written
    in the original program (hence why `previously` and `eventually` are written
    the way they are). It can be used to express more complex interleavings of
    temporal properties by having things before / after it as well.
  * `ANY` takes an argument (`int`, `long long`, `ptr` etc.) to represent the
    type of argument being passed to the function call.
  * The `call` function takes a single parameter - the function call as it would
    be written in the program! This is different to `returnfrom`, which takes an
    extra parameter for the return value. Why can't you do
    `call(foo(ANY(int)))`? Ask Jon.

## 5/12/2016

* Did some more digging into why explicit automata descriptions aren't working
  for me. No real answers yet - the symptom is that I have an undefined name in
  the instrumented LLVM code. Something has inserted a global variable for the
  automaton into the code but has not "expanded" it with a definition. Have
  emailed Jon for clarification / help but no answer yet. The next place to look
  is probably trying to find out what is meant to replace the global variable.

## 15/12/2016

* Still no reply from Jon. Should get Robert to prod him. Looking into why I get
  these linker errors with a much-simplified example. Seems that the
  `tesla_update_state` function is getting called with `my_auto` as an argument,
  which is a global variable in the LLVM IR. This means (probably) that
  something isn't actually populating this global variable with a real value.
  Dig though code and find out where?
* When we end up calling `ExternAutomatonDescrip` to get a global variable,
  the first time we call, there isn't a variable ready. The second time, there
  is and we get it.
* Looks like `BuildAutomatonDescription` is doing what we want but it isn't
  getting called - it is responsible for getting rid of extern specified global
  variables and replacing them with things that actually have an initializer!
* It seems to be called in two separate places in Assertion.cpp - how do these
  different places get triggered? Actually only in one location for the version
  of TESLA I have on my machine (why is this different??) This was different
  because I was building against CTSRD TESLA rather than CADETS TESLA and there
  was some difference in the code for newer commits.
* The path that gets to where we want to be is returning early because there are
  no assertions in the module. How do I add an assertion to the module?
* `TESLA_STRUCT_AUTOMATON` is dead - the actual syntax is to use the automaton
  just like a function call (for example, `eventually(my_auto(a))`)
* Usage: you can't go arbitrarily far down the call stack using `TESLA_WITHIN` -
  it seems to be only within the current function that it's usable. 
* Setting up: managed to get my tesla experimentation work shoved into the TESLA
  source tree so that I can build etc all in the same place. Took a bit of
  working to get CMake not to break but now it works. Next step: modelling
  simple locks.

## 16/12/2016

* Begin to work on getting a very simple model of locks verified. Set up project
  structure etc. Convenience macro for TESLA struct field accesses (might be
  extensible in the future to a full struct macro for ease).
* Implemented a simple lock automaton that ensures that a particular function is
  'well-behaved' with regard to a particular lock (i.e. it acquires the lock at
  some point, then releases it later). This currently catches things like
  releasing without acquiring, acquiring after release. It can't handle a
  deadlock situation yet (can it?). 
* Need to look into why if a thread exits without calling `lock_release`, it
  won't trigger an assertion failure.
* Using previously instead of eventually seems to fix the deadlock problem on
  the lock automata. Also added a check to make sure that acquire calls go
  "F...FT" only.
* Using two or more locks: I think strict mode might be a problem here. How to
  fix this?

## 17/12/2016

* Looking into how I can use `tesla-print` to get `.dot` files for debugging.
  Managed to get graphviz on OS X to output me a file. Workflow is a little bit
  inconvenient though (but obviously no way to view pngs on the server).
* Currently doesn't look like there's an easy way to have a function that acts
  as a thread worker be instrumented from outside of its own definition (or a
  caller), nor for a function to work on *multiple* locks.
* Even shifting stuff one level up the call stack seems to break things a bit -
  why can't I use `acq_rel` from `thread_work` which just calls into
  `thread_say`?
* Seem to have sorted this out - if I use the wider bound of `thread_work` then
  it works, but not the narrow bound of `thread_say`.
* Now have a system for making assertions about several locks in the same
  function (not interleaved though I don't think). Uses a strict sequence.
* At this stage I have a working verifier for a very simple lock that can be
  used to verify acquire-release behaviour for a single lock, multiple locks in
  sequence, or multiple locks interleaved.
* Next step is to think about how best to statically analyse this type of
  structure, and also to look at how locks are implemented internally in (for
  example) FreeBSD (and then to come up with equivalent automata for the real
  locks).
* Also worth thinking about what properties (in a formal sense) this
  implementation allows me to verify.

## 18/12/2016

* Small amount of work looking into the lifetime automata but to no real
  progress - I can get them to compile but their definitions seem to somehow be
  different to regular 'in-function' automata. Shelving this for now.

## 19/12/2016

* Begin looking into how static analysis can be performed on TESLA automata,
  starting with the example of a single lock being acquired and released.
* Structure of a TESLA automaton is described at the highest level by the
  protocol buffer specification ultimately. This gives us the "grammar" by which
  an automaton is described. Can see examples of this as the ouput from the
  `tesla-cat` step of building a program against TESLA.
* Obviously code for working with an automaton in this state is generated by the
  protobuf build step (so should look up how to actually work with this
  generated code).
* A `Manifest` describes all the automata contained in a single file, while an
  `Automaton` is singular (only describes one single set of states and
  transitions).
* Initial effort will be to statically analyse the locking assertions I've built
  over the last few days.
* What do we want to prove about this automata specifically? (in this case I
  think that I do actually want to be specific and see how it can be generalised
  to a more significant structure).
  * The `acq_rel` automaton *should* be generic over any kind of lock simple
    enough to be described with simple atomic acquire / release semantics.
  * For example, a Pthreads mutex could be adapted to this structure by wrapping
    the Pthreads acquire / release functions (should try this at some point by
    making an *almost* identical example to the current `locks.c` experiment.
  * What `acq_rel` asserts is that on any code path, the function will spin
    around 0 or more times failing to acquire the lock, then successfully
    acquires the lock, then releases it.
  * Need to characterise *exactly* what behaviour is allowed and forbidden by
    the automaton as it is currently implemented (by building a set of smaller
    test cases that demonstrate its behaviour - note that `lock_acquire` and
    `lock_free` are *mockable* - they present an interface that can just as well
    be implemented by something that doesn't actually do any locking at all!).
  * Then once I have an informal but demonstrable model of the lock automaton's
    behaviour, I can start to work out exactly how these properties can be
    described (and proved) at the IR level.
  * For this specific analysis of my automaton only, there is no need to really
    work with the TESLA IR at all - instead need to just look for the very
    specific form that instruments an acquire / release cycle.
* Worked out some CMake hackery to allow multiple TESLA executables to share a
  single source file. As far as I can see the solution of building every source
  file for every target executable is necessary - instrumentation will change
  things etc so this is the only way to do it generically (without having custom
  TESLA invocations for every single executable being built). These improvements
  to the TESLA build system allow me to build arbitrary C executables with TESLA
  assertions built in just by calling `add_tesla_executable`.
* Have also begun to do some rebasing and cleanup on the locks branch so that
  when I merge it to master it's not totally horrific to look at.
* Implemented a few small misbehaving programs to demonstrate the kinds of
  behaviour that the lock automata attempt to catch.

## 20/12/2016

* Finished off writing example programs that show the behaviour of my simple
  lock automata (successful / good path as well as several 'failed' examples).
  Still running into the issue that assertions don't seem to care about the
  identity of the pointer they're being called with (`acq_rel(&lock)` is
  equivalent to `acq_rel(&other)` behaviourally - it doesn't seem to matter
  which one is actually acquired and released). Need to work out if there's a
  solution to this (my instinct is that there isn't an easy way to do so because
  of the way instrumentation is added to code).
* One possible hacky workaround is to add a global ID to every lock - then, the
  `lock_X` functions could work on a lock ID rather than directly on a pointer.
  I think this might be easier to reason about, provided that the correct
  guarantees are made of the global lock ID 'repository'.
  * `lock_init` would need to increment the global ID counter, set the field on
    the lock, put the lock itself into an array (by value), then return the id.
  * `lock_acquire` and `lock_release` would need to work on the base pointer +
    offset, rather than on the addresses directly. They could then be modified
    to take an ID (integer) rather than a direct pointer.
  * `lock_free` would then need to perform any necessary cleanup (can't see what
    this would actually be just now).
  * This would mean that initializing a lock would not necessarily be atomic
    (but that's fine I think - just need to do lifecycle things sequentially
    before using locks for real).
  * Doing this might even make it easier to statically analyse locks - working
    out the value of an integer value seems like it would be more amenable than
    chasing pointers.
* Update: this strategy of using IDs rather than pointers isn't totally workable
  with the current implementation. Maybe possible in the future to rip it out
  and design from scratch (but wouldn't be able to use automata quite as
  idiomatically as they currently are).
* So with that in mind, the properties we can observe the `acq_rel` automaton as
  having are (for a *single* lock being used by a function):
  * `lock_acquire` returns false 0 or more times, until it returns true exactly
    once. It is not called again.
  * `lock_release` is called exactly once.
  * Any extra calls to either lock function will fail.
  * Calling `lock_release` before `lock_acquire` has returned true will fail.
* Where in the TESLA workflow should static analysis live?
  * Likely to be in the form of an IR pass to actually perform the analysis (but
    also need some information from the analyser level to know where to apply
    analysis).
  * For now, the analysis will be specific to the `acq_rel` automaton, and so
    all we need to do at the analyser level is find usages / bounds / locks etc
    for each instance of the automaton.
  * So at the point at which static analysis is performed we require:
    * The combined manifest file with the whole-program view of assertions made
    * The uninstrumented `.ll` file that the program was compiled into
  * From these prerequisites, we should then produce a statically-analysed
    version of the manifest that has had verifiable assertions removed (but that
    remains in the same format).
  * The optimised manifest can then be used as a drop-in replacement for the
    non-optimised version (bonus from this is that the program can then be
    instrumented using both versions and their behaviour compares).
* Gist of this is that what we want is really a new TESLA tool (`tesla-static`)
  that takes an bitcode file and a manifest for analysis, then returns a new
  with possibly different contents.
  * Benefit of this is that we're not doing anything unsafe at the bitcode level
    (in this particular model). All we do there is analysis of some kind that
    informs the different manifest.
  * Probably need this to be whole-program analysis. Should therefore
    investigate whether `llvm-link` can be built into the build system (i.e.
    that it plays nicely with instrumentation etc.)
  * It can in fact. I've now updated the TESLA build script to link all the .bc
    files into a single one before instrumenting (and have changed extensions
    etc. to be more correct). Now the TESLA build script will compile the
    program from a single .bc file (and analysis can be done on this
    individually).

## 21/12/2016

* Got the boilerplate set up for the static analysis tool (very similar to
  `tesla-instrument`). It is now able to load a manifest and an IR file from
  disk when it is run.
* Did a bit of tinkering to see what information I can get out of the manifest
  file using the protobuf generated code etc. Seems to be a reasonably easy
  interface to work with (`set_X` for setting fields, `X` for getting fields).
* Now need to work out what kind of interface I want the static analysis to
  have.
  * Will make sense to have a modular system (so that future analyses aren't
    tied directly into the structure of the tool itself and the initial work I'm
    doing here).
  * Then each additional analysis I do can be plugged into the tool with a
    minimum of work.
  * The information that we have at the start is the unmodified manifest file
    and the IR. It makes sense that each 'pass' will get the same things passed
    to it, with the exception that the manifest will be changed between each
    step (as it wouldn't make sense to leave it unchanged and then do every pass
    at once at the end).
  * So it will make sense to have:
    * Base class for manifest passes that defines an interface each conforms to.
    * Builder class that takes a list of instantiated manifest passes and runs
      them one after another in a pipeline.
* Have implemented a pass system that allows for manifest passes to be
  constructed and run on manifests / IR files.
* Now the next problem is how to create a new manifest given the original one.
  Seems to be that modifying in place isn't possible as the `Manifest` class is
  const-ed up pretty heavily. Should look further at `tesla-cat` to work out how
  this could be done. Looks like `ManifestPass::run()` will need to actually
  return a new `Manifest` (which can be passed into the next one in the pipeline
  etc.).
* The analysis for `acq_rel` is an include / exclude decision. So what needs to
  be done is walk the root automata for things that are `acq_rel` (if it's
  included), then delete (don't pass on) root automata if the analysis succeeds.

## 22/12/2016

* The `cat` tool generates a new file by directly writing the textual
  representation of a `ManifestFile` to disk. It seems to be valid to initialize
  a `Manifest` only given the file containing the textual representation.
* The logic to do this is pretty complex, but what we're going to want to do is
  hijack the process at the point at which we have a buffer containing a
  `ManifestFile` protobuf object.
* By doing this we then have a workflow:
    immutable Manifest --analysis-> ManifestFile --load--> new Manifest
* This is then composable for many passes in sequence, and `tesla-cat` shows how
  to generate a `ManifestFile` given constituent parts (Automata and Usages).
  From this we can then implement the recursive walk accept / reject style that
  will be used for the `acq_rel` analysis.
* Have replaced some parts of the `Manifest` code with uses of
  `std::unique_ptr`, as the legacy LLVM reimplementation has been deprecated. It
  would be worthwhile to fix up the rest of the code.
* Also another quality of life problem - there's a problem with how `llvm-lit`
  is running tests (to do with an output option), so probably wise to fix that
  before I do any major reshuffling of TESLA internals.
* So now we have a way to construct a `Manifest` from a `ManifestFile`. We don't
  have direct access to the input `ManifestFile`, but what we do have is the set
  of defined Automata and the set of root Usages. We probably don't want to ever
  get rid of a defined automata (as if there's no references to it then it will
  disappear at instrumentation time).
* That means that we need to copy all the defined Automata into a new
  `ManifestFile` regardless of what else is being done.
* Once the definitions are copied, it's then a case of walking root usages (how
  to handle sub-automata). This is at least the case for the include / exclude
  decision we're making in this model at first.
* So a the run method of each pass will actually need to return a new manifest
  (pointer to).
* Hygiene: pass interface is *bad* at the moment. Should rewrite to eliminate
  usage of raw pointers (think about ownership semantics). Fixed this for the
  most part, but there's still the interface between `main` and
  `ManifestPassManager` that has a couple of `shared_ptrs` sticking around.
  Should work to eliminate those as well.
* At the moment, a manager runs passes and resets its own `Manifest` (this
  allows for an 'ownership chain' through the run function). Should also maybe
  clean up the external interface a bit as everything is public at the moment.
* Next up it looks like some kind of generic way to walk over the *usages* in a
  manifest and make some kind of decision about them. Obviously the protocol
  buffer format gives us a handy way of describing the manifest as a tree, but
  what would be the best way to iterate over it?
* Alternatively, don't do it generically for now. The `acq_rel` automaton should
  be fairly easy to recognise usages for. We want roots that:
  * Are a sequence of events beginning with the assertion site, immediately
    followed by a subautomaton with name `acq_rel`, followed by a return from
    some function.
  * Can recognise things in this format by doing some pretty nasty protobuf
    code, but it's probably simpler for now to do that than to wrangle up some
    crazy generic method to do it.
  * Then once we have a way of recognizing usages of `acq_rel`, the next step is
    to pick out the bounds of each usage and do the analysis on the IR.

## 23/12/2016

* Today: work on recognizing usages of `acq_rel` in an automaton root. Want a
  function that takes a root usage, and returns true if it is an instance of
  `acq_rel`.
* Came across an interesting problem where instrumenting a module fails when all
  of the roots are removed from the file (fails in InstrContext.cpp with an
  error relating to automaton lifetimes).
* Have now built the static analysis tool into the TESLA CMake build
  (optionally). It can now automatically run the static analyser over the
  genrated manifest and compile two versions of the binary.
* So we actually need to look at all of the automata descriptions first off, and
  identify which automata use `acq_rel` (rather than looking directly at the
  usages as I previously thought).
* Strategy: search for automata that reference `acq_rel` in the way we define
  (sequence of events given above). Then populate the vector of locations
  appropriately. From these locations, iterate through the roots and check each
  location for membership in the set. If it is in the set, flag the beginning
  and end locations for analysis.
* So now have a pass that manages to remove *any* usages of `acq_rel` that it
  finds, but it still has the problem from above (that lifetimes get messed up).
* Should also work out a better way of getting myself out of a 'trusting trust'
  cycle. If work on `tesla-static` introduces a bug, then things start to fail
  *badly* at build time because it's part of the build process. Intermediate
  could be to use the CMake binary dir version?
* Actual solution turned out to be to use an extra CMake config variable to
  toggle static analysis of built binaries on or off. So if `tesla-static`
  introduces a build error, the solution is now to run `soff` so that the broken
  binary doesn't get run.
* Is the lifetime error limited to when there are no roots at all in a manifest?
  Confirmed that this seems to be the problem. Rather than spleunking through
  the original TESLA code, it might in fact be easier to do a cleanup pass that
  finds unused automata in the manifest and deletes them.
* The problem then with just deleting them is that calls to
  `tesla_inline_assertion` will then fail. Need to think about how best to deal
  with this problem going forward. Some options:
  * Extend protocol grammar to mark usages as deleted, then ignore them at
    instrumentation time, leaving the rest of the machinery intact.
  * Move away from manifest pass structure for now and use regular LLVM
    machinery to achieve the same idea.
* Worth nothing that currently I still don't know the actual cause of the
  lifetime problem.
* Final thoughts: I think the deletion marker is the best way to go without
  touching an LLVM pass as well. Benefits are:
  * No more messing around with these weird lifetime errors
  * Not throwing away information as the current approach does - all it's doing
    is *adding to* the manifest data to suggest approaches to the
    instrumentation pass.

## 13/1/2017

* Back working on TESLA after a break to do coursework post-christmas. Meeting
  with Robert earlier today in which we discussed the work that I've been doing
  so far on the project. Main points were:
  * He agrees that the problem of not being able to use the same automata on two
    different pointers is perplexing - should do some more digging into this and
    work out the actual source of the problem. Mentioned a table of parameters
    that can be indexed into to find the correct automaton instance for a call.
    This code might be in the runtime library.
  * Modelling the automaton using function calls to acquire and release is
    probably best as it abstracts the lock internals from the semantics of the
    action.
  * Think about how the model devised could be broadened to (for example)
    reader-writer locks, or data structures that permit operations only when
    locked.
  * Intra-procedural analysis and the placement of `tesla static` seem to be
    reasonably well thought out for now, as well as the proposed extensions to
    the instrumenter and the protobuf structure.
* Next steps to think about for the project:
  * Work out how to do the data flow analysis for simple acquire / release locks
    as I've implemented (will involve writing an LLVM pass).
  * Related: look into updating the protocol buffer specification so that I can
    add my own fields for the instrumenter to look at.
  * Should probably split out sections of this log that are too 'stream of
    consciousness' into separate documents (designing components like the data
    flow analysis is a good candidate).
  * Look into the problem / bug / misunderstanding relating to using multiple
    pointers with the same automaton. If no solution then ping Jon on Slack to
    get his thoughts.
* Progress today:
  * Fixed the systematic errors with the test suite, and have identified a
    possible regression in the parser.
  * Parser regression notes:
    * ParseArg -> MemberExpr branch -> ParseStructField -> ParseArg -> ???
    * Need to figure out what the next branch is!
    * Left a TODO in the right place to pick up where the error seems to
      manifest itself first. Should look at why the incorrect branch makes two
      goes round the function!

## 14/1/2017

* Fixed the parser regression identified yesterday. The solution was to add an
  extra parameter to the argument parsing methods so that they can be
  specialised to the case where we're passing a whole structure by value, but
  only care about a single field.
* Next step for this afternoon's work: look into the multiple automaton stuff
  and see if I can get that to work.
* Looked into strict / conditional mode in more detail. Think I have worked out
  (at least partially) what the problem with using multiple locks is (related to
  strict mode).
* Wrote up findings for the strict / conditional mode stuff.
* Starting to look at lock behaviour but have run into another parser issue (I
  think, anyway) - indirection seems to be be going wrong for me.
* Another useful command would be to disable experiments in general (eon / eoff)
  so that TESLA can be rebuilt and installed easily if the experiment build is
  broken, without having to change code.

## 15/1/2017

* Added the experimental on / off flags that allow TESLA to be rebuilt and
  installed if the experiments are breaking the build.
* Looked into the indirection issue from yesterday, which seems like it could be
  a known limitation - sent Jon a message asking for clarification.
* Wrote up current status of locking automata and informal / behavioural
  specifications of the properties that it asserts.
* Integrated the new protocol buffer field into `AcquireReleasePass`. As a first
  / dummy operation it just searches for usages of the automaton named, then
  adds the deleted field before copying the usage across.
* Next step is to design the actual optimisation that informs the copy yes / no
  stage of the pass.
* Extended the instrumenter so that it will skip over deleted usages without
  adding any extra instrumentation. Tested this on the locks experiments, and in
  simple cases it seems to work (allows the failing cases to pass when their
  usages of `acq_rel` are deleted).
* Looked into the usage of field assignments as assertion bound events.

## 16/1/2017

* Looked into how the variable names in an assertion site are mapped onto LLVM
  `Value` objects. Found the mechanism that does this by searching all of the
  things in scope at the assertion site (allocas, globals and parameters).
* Worked out where the actual static analysis will live inside the
  `AcquireReleasePass`, and when it will get called.
* Started to stub out some implementations / things that will need to be done
  to get the appropriate information out of the usage and module.
* Fixed that annoying CMake install problem for the experiments directory.

## 17/1/2017

* Next step is to rip out the part of the instrumenter that maps variable names
  onto an LLVM `Value`. Have implemented this and moved the appropriate parts
  out into a common location so that the static analyser is able to access them.
* Working on integrating the LLVM pass boilerplate into the setup I have going
  at the moment - have it such that the pass is run, setting a variable on
  itself that is checked by the consumer of the pass afterwards.
* Realised that my pass construction interface isn't quite right, and I need to
  actually give an `Automata`.

## 18/1/2017

* Rewired stuff to pass an `Automaton` instance into the LLVM pass so that the
  argument information can be accessed.
* Run into a problem with how arguments are stored on an `Automaton` - need to
  recurse down into subautomata.
* Worked it out so that the LLVM pass can be given the argument list from the
  subautomata. In the future (i.e. when thinking about generic analysis), it
  will be useful to work out a way of extracting the argument vector from an
  automaton's subautomata more generally. Given that each has an index, I can
  see it being possible via a subautomata walking approach.
* Implemented the first actual static analysis of the project! I can now check
  `acq_rel` assertions to make sure that on every call path in the bounds, calls
  to the acquire and release functions *only* use the variable named in the
  assertion.

## 19/1/2017

*  Big achievement for the day is getting the project to build without errors on
   LLVM 3.4. It doesn't look like it'll be easy to get it updated further
   because of incompatibilities with the CMake system that I haven't been able
   to resolve yet.

## 23/1/2017

* Worked out how to get call graph information out of the corresponding LLVM
  pass.
* Realised that some of my thinking on how to improve the 'other lock' analysis
  was actually wrong.
* Begun to implement a more general framework for implementing static analyses
  within the context of an LLVM pass. The idea is that the LLVM pass has a
  larger-scale analysis it wants to do, and the individual analyses do the
  smaller units of work.
* Added some TODOs and questions about the static analysis methodology, and the
  `OtherLock` analysis.

## 24/1/2017

* Finished factoring out the "Other Lock Used" static analysis into its own
  self-contained implementation. The next step will be to write more of these
  analyses.
* Added a new example that doesn't check that acquisition was successful.
* Begun to work on the analysis that will check for non-branching on the result
  of calls to the acquire function.
* Maybe worth noting that because the `acq_rel` analysis is so specific to this
  one automaton, it is possible to generate warnings.
* Found a limitation of the interprocedural approach taken here - can't really
  go back to a file name once a debug location is identified.
* Finished implementing the "no branch" analysis.
* Added a failing example for the case when a lock is released before being
  acquired, and begun to work on the analysis of this behaviour.

# 25/1/2017

* Today working on the analysis that checks for release-before-acquire of a
  lock. Implemented this and seen it working on the example written yesterday to
  demonstrate the problem.
* Implemented a common way to get the debug location from an instruction
  (module, function, line), and converted existing analyses to use it.
* Spent some time in debugging hell trying to get the CallGraph to work, only to
  discover it was all my fault in the end anyway. Or is it? Still getting
  bizarre results.
* Maybe worth reinventing the wheel after all and building a 'SimpleCallGraph'
  structure that can just build itself from a module. Saves lots of time
  hassling the LLVM stuff to work.
* Begun to work on this data structure to save on LLVM pain.

# 26/1/2017

* Implemented the simpler call graph structure and started to use it in the call
  order analysis.
* Realised that my approach to this is actually a bit wrong and have had to
  think about how to improve it (problem is ordering, really).

# 27/1/2017

* Rethought some of the problems identified yesterday (with dominance vs. call
  graph analysis), and realised that it is possible to reconcile the two into a
  useful analysis.
* Meeting with Robert - progress continues and some interesting ideas and
  thoughts have been written up as notes.
* Finalised the call order analysis with a useful warning message.

# 28/1/2017

* Noted that using a lock asserted about only once is covered by the RBA
  analysis.
* Realised that I'm missing some very simple analyses that will catch mistakes
  in usage (e.g. missing calls, release dominance).
* Beginning to implement these analyses to cover all the examples.
* Worked out that what I want is reachability rather than dominance in the cases
  I've used so far.
* Begun to implement a way of computing BB reachability within a function.

# 29/1/2017

* Implemented the reachability graph check for getting from one basic block to
  another.
* Finished implementing the release reachability analysis that makes sure we
  can't call release again after calling it once.
* Begun to look at an analysis that looks for address-taken instances of the
  acquire and release functions.
* Looked into some informal performance analysis of optimised apps - learned to
  use gprof for this. Needs a more formal analysis to take into account the
  large variation in results, but generally can see an improvement when using
  the optimised version. A next step would be to fix an actual benchmark and get
  some results into a graph etc.

# 30/1/2017

* Identified that the multiple acquire example actually does need the FF...T
  analysis example to be recognised (as before it was in a weaker form that
  could be detected by the no-branch analysis).
* Implemented a chunk of the tracing algorithm that will eventually map calls to
  acquire onto a branch, with a modifier to choose the destination.
* Finished implementing the FF...T analysis, and thought about some bugs that
  are picked up by this that inform a couple more small analyses.
* Started to think about benchmarking and profiling an example to show the
  benefit of removing locks.

# 31/1/2017

* Looked more into how callgrind can be used to get program costs - should
  confirm this with Robert as a viable benchmarking method. Collected the data
  from the callgrind runs for 10 threads.

# 3/2/2017

* Meeting with Robert - chatted about benchmarking strategies for synthetic
  examples for the mutex analysis. Callgrind seems viable for simple examples,
  but for more complex ones it might affect the concurrent execution too much to
  be useful. For a better analysis, a real computation might well provide a
  useful measure via wall clock time (rather than just spinning).
* Asked Jon where to get an updated FreeBSD kernel with TESLA assertions so that
  I can try to reproduce the performance analyses from the original paper.
* Begun to have a think about how we can do more generic analysis of automata,
  including a small writeup of the key things we'll need to look at and some
  questions that should be answered.
* Chatted to Arun about building FreeBSD with TESLA enabled.
* Starting to look into compiling a FReeBSD kernel that I can add things to and
  instrument etc.
* Ran into some errors when compiling FreeBSD.

# 4/2/2017

* Added methods to the Manifest class that allow for automata search that
  doesn't panic if they aren't found. Updated current code to use these new
  methods if possible.
* Fixed some bugs in the acq-rel analysis that caused crashes when not running
  on the expected examples.
* Started to think about call sequence analysis.

# 5/2/2017

* Trying to build the CADETS version of the kernel rather than the TESLA
  version. The problem I was experiencing yesterday seemed to be to do with
  there not being a rule to make a particular .tesla file for a .c file, so
  maybe have a look at the makefile that gives the rules for TESLA builds, as
  well as the one that builds cam.c.
* Worked out some properties of functions that are useful to prove to perform
  static analysis (calls exactly once etc).
* So it's definitely TESLA causing the problem with the kernel build - the
  vanilla CADETS version builds just fine.
* Implemented some auxiliary stuff getting towards a calls-exactly-once
  analysis.
* Finished implementing the calls-exactly-once analysis.
* Did a lot more digging into the Makefile problems that are stopping me from
  being able to build the kernel with TESLA enabled.

# 6/2/2017

* Spent lots of time tracking down an annoyingly subtle bug in the code for
  transitive-once-calls.
* Had an interesting idea - what if I were to extract a common format from both
  code and assertions, then use more general theorem proving techniques to prove
  properties of the program? Has other benefits like counterexample generation
  etc. Definitely worth looking into in some detail.

# 7/2/2017

* Wrote up a starting note about the model-checking approach.
* Started to do some background reading on model checking for C programs.
* Implemented a non-spinny benchmark for locks - have threads do a bubble sort
  on an array interval while holding a global lock on the array.
* Used the non-spinny benchmark to do some back of the envelope data gathering.
  It seems like the statically analysed and optimised version is indeed reliably
  faster than the instrumented version. Need to verify, but it seems like the
  difference increases with contention as would be expected.

# 8/2/2017

* Collected some actual performance data for the instrumented vs. uninstrumented
  versions of the acquire-release automaton.
* Kicked off a rather long-running data acquisition script that will (hopefully)
  get some good data for the static / non-static versions.
* Reviewed a few of the papers identified as possibly being relevant.
* Fiddled with benchmarks to try to get better data.

# 9/2/2017

* More fiddling with benchmarks to try to get more compelling data.
* Continued with lit review.
* Noted a 500 byte / 20% reduction in binary size when applying static analysis
  to the locks benchmark.
* Begun to work out what I want a model checking tool to be able to achieve.

# 10/2/2017

* Meeting with Robert. Worked out that Jon should be able to log into a CL login
  server, and from there take a look at my failing FreeBSD compilation. Should
  also ask him where I get my hands on the OpenSSL code so that I can look at
  real software that uses TESLA assertions.
* Robert agrees that the model checking approach is viable, especially at the IR
  level. He mentioned that data flow analysis is important - i.e. if several
  data structures are being instrumented, how do we know statically which one
  we're proving properties about?
* Need to think about how best to skip stuff in a TESLA automaton -
  counterintuitively, the best way might actually be to add *extra states* to an
  automaton - this way, we can encode transitions directly past stuff that we
  know to be true. More digging through the TESLA code needed here.
* Did some stats on a promising benchmark - T test seems to indicate that at
  higher levels of contention, there's a significant improvement when using the
  statically analysed version of the code. Difference is less significant when
  there's less contention, but that makes sense as the overhead is all in code
  that only gets triggered on lock contention.
* Have kicked off a larger benchmark.

# 13/2/2017

* Worked on model checking implementation - started to flesh out data structures
  and look at what format the data will be in.

# 14/2/2017

* Big chunk of work on model building - can extract a model from basic blocks
  and functions that shows event flow.
* Worked out a better way of structuring the code for dealign with graph
  construction and began to rewrite the initial prototype using the new
  techniques.
* Written an instruction graph finder for an individual function.

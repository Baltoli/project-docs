In this chapter I evaluate the success of the project with respect to its
initial goals, and examine how future work could improve on what has been
achieved.

\section{Static Analysis of Assertions}

The primary goal of this research project was to investigate to what extent
TESLA assertions can be checked at compile time, with a view to performing
optimisation of instrumented programs by omitting provable assertions. With
respect to this goal, the project has been successful.

\subsection{Contributions}

My primary contribution with regard to static analysis of TESLA assertions is
the TESLA model checker---a significant additition to the existing
TESLA toolchain. The assertion language used by TESLA is able to express
temporal properties not expressible in comparable tools; static analysis of a
subset of these assertions is therefore a useful and novel development in the
field of program verification.

Being able to omit provable automata using the model checker leads to
significant performance improvements, making TESLA a viable tool for
instrumenting production code. Additionally, counterexamples generated by the
model checker provide a valuable debugging tool to help developers understand
the situations in which their assertions may fail---even if an assertion is not
provable, being able to demonstrate situations in which it can fail is useful.

\subsection{Performance}

Running the model checker is computationally expensive---the number of execution
traces examined depends on both the length bound and the complexity of the
control flow graph. However, because traces are examined in order of length, a
minimal counterexample will be produced as quickly as possible if one exists. In
practical terms, this means that the model checker will only run for a long time
if an assertion has no counterexamples. For example, checking any incorrect test
case from \autoref{sec:hand-coded} with a bound of \num{1000} basic blocks takes
less than \num{0.1}\si{\second}, while the successful cases take approximately
\num{125}\si{\second}.

This informal observation relies on the hypothesis that counterexamples are
almost always minimal. I argue that this is the case for automata that do not
place finite upper bounds on sequence repetition---if this holds, then the size
of the automaton (and therefore of minimal counterexamples) is bounded by the
number of separate statements in the automaton. A full analysis of automaton
structure may be able to formalise this argument.

Model checking TESLA assertions is likely to remain an \textquote{offline} tool
for developers---for development builds, the runtime overhead of TESLA
instrumentation is likely to be acceptable, while running the model checker for
several minutes is not likely to be. For release builds, running the model
checker for a long period of time (by a continuous integration tool, for
example) would be an acceptable trade-off to improve runtime performance.

Informal analysis indicates that the complexity of the model checker is
polynomial in the number of basic blocks bounding the counterexample length.
Performance can be significantly improved in two primary ways:
\begin{itemize}
  \item LLVM implements a control flow graph simplification pass that can
    potentially reduce the number of conditional branches. Running this pass
    after the inlining process led to a significant performance improvement.
  \item Narrower bounds for assertions will naturally lead to less complex
    control flow in the inlined function---this is dependent on the author of
    the assertion rather than the model checker itself.
\end{itemize}

\subsection{Correctness}

The TESLA model checker implements a decision procedure (\textquote{is this
assertion usage safe to remove?}). False negatives result in redundant
instrumentation code being left in a program, while false positives result in
dynamic checking being unsafely removed. As a result, the model checker must be
completely free of false positives in order to be correct.
\textcite{livshits_defense_2015} advocate for \emph{soundiness}\footnote{In
short, soundiness is \textquote{soundness modulo openly-stated exceptions}.} as
a goal of program analysis. In this spirit, I acknowledge that the TESLA model
checker is unsound in two important ways: trace length and inlining depth. If
these two parameters are sufficiently large, then the analysis I present is
sound.

During development, any false positives were treated as serious bugs. During my
instrumentation of the \lwip{} library interface (and other work not included in
this dissertation), I encountered only a small number of these, all of which
were fixed.\footnote{Some \textquote{unsafe} behaviour remains because of bugs
in the original TESLA implementation.} Future work on testing the model checker
could use techniques from automatic test generation or fuzzing to ensure no
false positives are found.

\subsection{Future Work}

Future work on the model checker could improve it in a number of ways. The use
of non-symbolic model checking is not optimal---adapting it to construct a
symbolic representation would allow it to take advantage of wider improvements
in the field of model checking. 

The TESLA assertion language could be extended to allow richer properties of
function return values to be expressed (e.g. \textquote{function \texttt{f}
returns a value $> 0$ and $\leq 10$}). SMT methods are well suited to this type
of expression. Another interesting extension would be to allow a non-sequential
syntax for describing complex automata explicitly, though integrating this with
the clang-based assertion parser would be difficult.

\textcite{kashyap_producing_2008} describe a method for discovering
\textquote{crucial events} in traces---these events can be used to perform more
intelligently directed search for counterexamples. Applying these techniques to
the model checker could improve checking performance when counterexamples are
not the shortest traces.

\section{Application to Real-World Code} \label{sec:eval-app}

A secondary goal of the project was to investigate how TESLA could be used to
verify the behaviour of a network protocol. In particular, performance-sensitive
code was of interest here in order to show the potential benefits of applying
static analysis to TESLA assertions. While this goal was not directly
successful, the investigation into how such code could be modelled led to an
explicit analysis of source code features that make TESLA assertions less
applicable, as well as to a general method for applying TESLA to library
interface code.

\subsection{Contributions}

The approach taken by \textcite{anderson_tesla:_2014} towards using TESLA placed
it firmly in the category of \emph{debugging tools}---that is, TESLA
instrumentation could be added to a program in order to diagnose bugs, but it
would be removed in a release build due to the associated runtime performance
overhead. The use cases described in the original TESLA paper are all
applications to individual programs or libraries where this debugging process
has been used successfully. However, it is worth noting that the original uses
of TESLA all involve instrumentation on system boundaries in much the same way
as the general usage I describe (though this is not stated explicitly in the
paper).\footnote{In particular, the boundaries identified are the OpenSSL X509
verification API, the FreeBSD MAC framework and the Objective-C message dispatch
mechanism.}

I contribute a more general usage of TESLA---library developers can use TESLA to
enforce usage properties of their code without having prior knowledge of the
user's code. This method is applied successfully to the \lwip{} TCP callback
library. While applying TESLA in this way is possible without my extensions for
static analysis, the performance impact of doing so can be prohibitive. In
\autoref{sec:eval-perf} I perform a detailed performance analysis of this
performance impact, and the possible improvements that can be made.

\subsection{Performance} \label{sec:eval-perf}

In \autoref{sec:safer-libs} I described an adaptation of the \lwip{} TCP interface
to allow for TESLA instrumentation, and demonstrated that it could be used to
build existing applications with almost no modification of their code. However,
adding TESLA instrumentation incurs a performance overhead for the application.

In this section I analyse the overhead of TESLA instrumentation in the context
of an echo server written using the \lwip{} TCP library. I show that even with as
few as five assertions, performance is degraded by $40\%$. Finally, I show
that applying the model checker to these assertions can reduce the overhead
significantly, and in some cases remove it entirely.

\subsubsection{Experimental Setup}

A useful benchmark for an echo server is to measure how many requests of a fixed
size it can handle in a fixed time period. An existing tool by
\textcite{hoyer_rust_2016} was used to perform this benchmark---the tool runs
for a fixed length of time, sending as many messages to a server as it can
(using a configurable number of threads).

Three versions of the echo server were compared in this experiment---an
unmodified one compiled directly from the \lwip{} sources, one with TESLA
instrumentation enabled, and one where TESLA instrumentation was removed by the
model checker (referred to as \emph{unmodified}, \emph{instrumented} and
\emph{static} respectively).

The benchmarks were run on a dedicated server (Intel Xeon E5-1620
\SI{3.6}{\GHz}, 8 cores, 64GB of RAM) running FreeBSD 11. The benchmark was run
for \SI{60}{\s} with a message size of 512 bytes in every case, and the number
of sending threads was varied from 1 to 10.

For the instrumented and statically analysed versions, the library wrapper code
contained five TESLA assertions covering the possible library calls that the
user code could make. All of these were reported as safe by the model checker.

\subsubsection{Results}

In all three version, throughput was saturated when sending on two or more
threads---using more threads to send data had no effect on throughput. The mean
throughput over 2--10 sending threads was therefore used as a measure of the
maximum possible performance of each server (relative to the unmodified server
implementation). 

\begin{figure}
  \centering
  \begin{tikzpicture}
    \pgfplotstableread[col sep=comma]{data/echo_bench_avg.dat}\data
    \begin{axis}[
      title={Relative Throughput of Echo Servers},
      ylabel={Throughput relative to unmodified server (\%)},
      xlabel={Server implementation},
      width=0.85\textwidth,
      ybar,
      ymin=0,ymax=110,
      cycle list/Dark2,
      every axis plot/.append style={fill,draw=none,no markers},
      ymajorgrids=true,
      symbolic x coords={Unmodified,Static,Instrumented},
      xtick={Unmodified,Static,Instrumented},
      %x tick label style={rotate=45, anchor=east},
      enlarge x limits=0.25,
      nodes near coords,
      bar shift=0pt,
      bar width=4ex,
    ]
      \addplot+ coordinates { (Unmodified, 100.0) };
      %\addplot+ coordinates { (Static (all), 99.4) };
      \addplot+ coordinates { (Static, 84.1) };
      \addplot+ coordinates { (Instrumented, 60.8) };
    \end{axis}
  \end{tikzpicture}
  \caption{Effect of TESLA instrumentation and library interface adaptation on
  echo server throughput}
  \label{fig:echo-bench}
\end{figure}

\autoref{fig:echo-bench} compares the relative throughput for each
implementation. The version with runtime TESLA instrumentation achieves only 61\%
throughput when compared to the unmodified version, while the version with
static analysis applied achieves 84\% throughput.

Even though all TESLA automata were reported as safe by the model checker, some
TESLA code remains in the binary. Using Callgrind \cite{weidendorfer_tool_2004}
(a simulation-based profiling tool) shows that the statically-analysed server
spends a large portion of its execution time in the TESLA runtime library
(performing redundant work).

Because the overhead of the remaining TESLA instrumentation is high, even when
there are no automata, I extended the instrumenting tool with an optimisation to
handle this case. The resulting server implementation achieved 99.4\% throughput
compared to the unmodified server. While this performance result is appealing,
it is worth noting that it will only be possible when every automaton is
statically provable.

\subsubsection{Analysis} \label{sec:perf-analysis}

It is clear from these results that TESLA instrumentation has a
significant impact on performance. In this section I analyse the causes
of this overhead by using the
\texttt{hwpmc}\footnote{\url{https://www.freebsd.org/cgi/man.cgi?query=hwpmc}}
tools available in FreeBSD.

Hardware performance counters are a set of specialised registers
available on modern processors that can be used to measure architectural
and microarchitectural performance events. For example, the number of
mispredicted branches or the number of cache line misses are commonly
made available. The exact data available varies between architectures,
and even between different models of processor on the same architecture.
FreeBSD makes these performance counters available to user programs
through the \texttt{hwpmc} driver and associated tooling.

Using these counters can have an effect on the performance
characteristics of a program. In order to verify that this effect was
acceptable, the experiment described above was repeated with five
performance counters enabled. There was no significant change in
relative performance when running with counters enabled---the maximum
deviation from the results given above was 1.7\%.\footnote{Absolute
throughput with counters enabled was approximately 90\% of the
throughput with no counters.}

The performance counters enabled were as follows: the number of
instructions retired, the number of load instructions retired, and the
number of loads from the L1, L2 and L3 caches respectively. Each counter
was run in sampling mode, meaning that data was obtained by statistical
sampling over all the events observed. Each server was sampled over the
course of a fixed-size data transfer, with the size of the data used
varying from 10--150MB.

An approximation to the amount of work performed by each version is the
total number of instructions retired.\footnote{An instruction is retired
when it has been executed and its effects written back---in a
superscalar architecture, instructions may be speculatively dispatched
but not retired.} \autoref{fig:retired-bench} shows the number of
instructions retired during these transfers for each server version.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      title={Effect of data transfer size on instructions retired},
      width=0.85\textwidth,
      xlabel={Data size (MB)},
      ylabel={Instructions retired},
      xmin=7,xmax=153,
      ymin=0,ymax=250000,
      legend pos=north west,
      ymajorgrids=true,
      grid style=dashed,
      cycle list/Dark2,
      yticklabel style={
	      /pgf/number format/fixed,
	      /pgf/number format/precision=5
      },
      scaled y ticks=false
    ]
      \addplot+[mark=x]
      table [x =size, y =unmod]{data/retired.dat};

      %\addplot+[mark=x]
      %table [x =size, y =all]{data/retired.dat};

      \addplot+[mark=x]
      table [x =size, y =static]{data/retired.dat};

      \addplot+[mark=x]
      table [x =size, y =instr]{data/retired.dat};
      \legend{Unmodified,Static,Instrumented}
    \end{axis}
  \end{tikzpicture}
  \caption{Number of instructions retired during a fixed-size data
  transfer}
  \label{fig:retired-bench}
\end{figure}

The number of instructions retired is proportional to the quantity of data being
transferred by the server. On average, the statically analysed server retired
$1.37\times$ the instructions of the unmodified server, while the instrumented
version retired $2.49\times$ as many. Further experiments with much larger data
files showed that this relationship continues to hold.

The memory access patterns for each version are almost identical---for each one,
28\% of the total instructions retired are load instructions, of which 94\% are
served by the L1 cache, and less than 0.2\% by the L3 cache (although because
the total number of instructions executed is greater, the total number of loads
from the L3 cache and cache misses will increase).

A similar analysis was performed for instruction cache misses, with the same
conclusion---TESLA instrumentation did not proportionately change the
microarchitectural performance characteristics of the server implementations.

The sampled counter data can be used to show where instructions are
retired during execution. \autoref{lst:instr-callchain} shows a sampled
excerpt from the instrumented server. From this data we can see that
TESLA automaton operations account for 37\% of the retired instructions,
with 25\% corresponding to state updates and 12\% to lifetime
management.\footnote{The TESLA terminology for creating and destroying
automata is ``sunrise'' and ``sunset''.}

\begin{figure}
  \begin{minted}{objdump-nasm}
@ INSTR_RETIRED_ANY [227749 samples]

36.66%  [83489]    strncmp @ /lib/libc.so.7
  100.0%  [83489]     same_static_lifetime @ libtesla.so
    68.99%  [57601]      tesla_update_class_state @ libtesla.so
    16.00%  [13358]      tesla_sunset @ libtesla.so
    15.01%  [12530]      tesla_sunrise @ libtesla.so
06.73%  [15337]    inet_chksum_pseudo @ tesla-app.instr
  ...
  \end{minted}
  \caption{Callchain output showing number of instructions retired with
  TESLA instrumentation enabled}
  \label{lst:instr-callchain}
\end{figure}

The equivalent data for the statically analysed server (in
\autoref{lst:static-callchain}) shows where the remaining overhead arises. Calls
to the automaton lifetime functions remain (with a similar number of associated
instructions), while the state update functions have been removed. Calls to the
lifetime functions can be removed only if there are no automata to be
instrumented.

\begin{figure}
  \begin{minted}{objdump-nasm}
@ INSTR_RETIRED_ANY [125375 samples]

22.71%  [28467]    strncmp @ /lib/libc.so.7
  100.0%  [28467]     same_static_lifetime @ libtesla.so
    51.17%  [14567]      tesla_sunrise @ libtesla.so
    48.83%  [13900]      tesla_sunset @ libtesla.so
12.74%  [15968]    inet_chksum_pseudo @ tesla-app.static
  ...
  \end{minted}
  \caption{Callchain output showing number of instructions retired with
  TESLA instrumentation removed}
  \label{lst:static-callchain}
\end{figure}

\section{Usability}

While not an explicit goal of the project at the outset, the usability of TESLA
as a programming tool is an issue that I encountered frequently during the
course of the project. I was able to resolve a number of these issues,
contributing to the usability of TESLA:
\begin{description}
  \item[LLVM] TESLA now builds against the latest stable version of LLVM, making
    it easier to install (a full source build of LLVM is no longer necessary on
    most operating systems). Future updates will also be made easier by this work.

  \item[Installation] I have simplified the TESLA build and installation
    process, allowing it to be installed using
    Homebrew\footnote{\url{https://brew.sh/}} on Mac OS X. Distributions for
    other package managers would also be possible.

  \item[Documentation] The TESLA documentation was somewhat out of date when I
    began my work---I have produced a new set of documentation that covers basic
    usage of TESLA, and notes many of the subtle issues I have experienced
    during my work.
\end{description}

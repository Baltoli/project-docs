In this chapter I discuss potential applications of statically checked
TESLA to practical software engineering scenarios. I provide an analysis
of how coding style can make writing TESLA assertions for a system more
difficult, with reference to a large open-source library. Finally, I
contribute a framework for using TESLA with static analysis to enforce
temporal invariants imposed by library code on users of the library.

\section{LWIP} \label{sec:lwip}

An initial goal of the project was to investigate how TESLA might be
applied to verify the behaviour of a larger state machine such as that
of TCP. However, this verification proved to be more difficult than
anticipated for a number of reasons.

In this section I investigate the application of TESLA to LWIP
\cite{dunkels_design_2001}, a widely used, portable implementation of
the IP protocol stack. I describe difficulties encountered in this
process with reference to the LWIP source, as well as an analysis of how
code written from scratch with TESLA instrumentation in mind could
mitigate these issues.

\subsection{Structure}

LWIP is distributed as a configurable library so that it can be built on
virtually any platform with a C compiler---interfaces to network
buffers, timers and other platform-specific code are abstracted so that
their implementation can be specified by users. Configurations for
widely used operating systems (generic Unix, Windows etc.) are
distributed as a secondary library together with example applications.

The core networking code of LWIP is around 57K lines of C.\footnote{Not
including header files or tests.} This code includes implementations of
IPV4, IPV6, TCP, UDP and a number of application-layer protocols. The
secondary library has around 13K lines of C, mostly contained in
implementations of executable server applications (HTTP, Telnet, SMTP
etc.).

\subsection{Investigation}

The goal of my investigation into LWIP was to instrument the core TCP
implementation with useful TESLA assertions, then to demonstrate that
performance improvements were attainable by applying static analysis to
this instrumentation.

\subsubsection{Building LWIP}

Before any TESLA assertions could be written, a version of LWIP built
using the TESLA compilation model described in \autoref{sec:build-tesla}
was required.

Each of the LWIP-based server applications is built using a Makefile
that compiles the core library separately, then links the
application-specific code with the core library. Modifying this build
system to use the TESLA infrastructure was not difficult, as much of the
setup (flags, includes, linking etc.) was in place already---the only
changes needed were to add the extra TESLA-specific rules and to compile
to bitcode instead of object files.

The end result of this modification was that any of the example applications
distributed with LWIP could be instrumented and built against TESLA.

I now provide an analysis of the LWIP TCP source code features that make
TESLA instrumentation more difficult.

\subsubsection{TCP State Implementation}

At the core of the TCP protocol implementation is a structure
representing a single TCP connection (\mintinline{c}{struct tcp_pcb}).
Almost all of the TCP protocol implementation is expressed in terms of
these structures---\autoref{lst:tcp-decls} contains some function
declarations taken from the source code that use the structure.

\begin{listing}
  \begin{minted}{c}
err_t tcp_bind(struct tcp_pcb *pcb, const ip_addr_t *ipaddr, u16_t port);

err_t tcp_close(struct tcp_pcb *pcb);

struct tcp_pcb * tcp_listen_with_backlog(struct tcp_pcb *pcb, u8_t backlog)
  \end{minted}
  \caption{Function declarations from the LWIP TCP implementation.}
  \label{lst:tcp-decls}
\end{listing}

Such heavy reliance on structure fields is obviously less than ideal
from the perspective of static analysis, although checking assignments
to structure fields is within the capabilities of TESLA without static
analysis. However, a further complication is that the implementation is
not consistent in its use of PCB structures---some functions modify a
structure passed to them, while others return an entirely new structure.
The latter style of function is far more difficult to instrument
effectively in TESLA.

\subsubsection{Macro Usage}

In order for LWIP to be universally portable, it makes heavy use of the
C preprocessor for a number of reasons. For example:
\begin{description}
  \item[Platform-specific implementations] The implementation of some
  functions can vary from system to system (e.g.\ endianness conversion
  functions). Macros are used to select the correct implementation of
  these functions without the overhead of a function call. This means
  that any TESLA assertions added to these functions would become
  platform-specific, and potentially duplicated between implementations.

  \item[Conditional Compilation] Almost every feature of LWIP can be
  enabled, disabled or modified at compile-time by setting the correct
  preprocessor definitions (this is what allows LWIP to be used so
  effectively on systems with limited resources). In LWIP, this feature
  is used in places to conditionally change the fields contained in a
  structure---any assertions written about that field must then be aware
  of the required \mintinline{c}{#ifdef} context.

  \item[Inlined Functions] Some simple ``functions'' in LWIP are
  expressed using macros to guarantee that there is no function call
  overhead, rather than relying on the compiler to inline
  them.\footnote{It is perhaps worth noting that LWIP was first released
  in 2001---the performance of optimising C compilers has improved a
  great deal since then.} These function-like macros cannot be asserted
  about by TESLA, and are difficult to distinguish in source code.
\end{description}

Taken together, these uses of the macro system make TESLA
instrumentation far more difficult to add to the LWIP source.

\subsubsection{Control Flow}

TESLA assertions are most useful (especially when using static analysis)
for asserting properties related to control flow events. However, the
style in which LWIP code is written means that there is little explicit
control flow within the protocol implementation itself---many functions
perform complicated work on a PCB structure, then call only a single
other function to send a packet.

In addition to the long functions and shallow call graph in the TCP
implementation, applying TESLA becomes even more difficult because of
the way users of the TCP implementation call into it---code that uses
the TCP implementation must register a set of callback functions that
are called at specified points in the protocol's execution.
\autoref{lst:callbacks} shows an extract from the TCP echo server in
which these callbacks are registered.

\begin{listing}
  \begin{minted}{c}
tcp_recv(newpcb, tcpecho_raw_recv);
tcp_err(newpcb, tcpecho_raw_error);
tcp_poll(newpcb, tcpecho_raw_poll, 0);
tcp_sent(newpcb, tcpecho_raw_sent);
  \end{minted}
  \caption{Callback registration for a user of the LWIP TCP implementation}
  \label{lst:callbacks}
\end{listing}

Registered callbacks are stored as members of a PCB structure. This
behaviour defeats TESLA instrumentation (both static and
dynamic)---there is currently no way to express \textquote{the function
\texttt{pcb.member} is eventually called} in the assertion language.
Unfortunately, these callback functions contain much of the behaviour
that would be well-served by TESLA instrumentation. For example, the
LWIP
documentation\footnote{\url{http://lwip.wikia.com/wiki/Raw/TCP\#Receiving_TCP_data}}
describes the mandated behaviour of a particular callback function:

\begin{displayquote}[LWIP Wiki]
When the application has processed the incoming data, it must call the
\mintinline{c}{tcp_recved()} function to indicate that TCP can increase
the receive window.
\end{displayquote}

Because of the callback interface, instrumenting the invariants of this
function could only be done by the consumer of the library (rather than
the author of the library). This means that the author of the library
can do little beyond documentation to ensure correct usage of the API
functions. In \autoref{sec:safer-libs} I show how this problem can be partially
solved using TESLA.

\subsection{Summary}

The LWIP TCP library presents an interesting target for verification with TESLA.
However, the style in which the library is written means that applying TESLA
assertions to the internal code is both difficult and unlikely to yield any
useful insight into the behaviour of the library. Additionally, the use of a
callback-based API for users of the library means that TESLA cannot be directly
applied in the situation where it would be most useful (enforcing temporal
assertions on user-supplied code).

\section{Safer Library Interfaces with TESLA} \label{sec:safer-libs}

In this section I describe the implementation of a mechanism by which a
library can use TESLA assertions to verify correct usage of the library
by clients. First, I relate the problem to the difficulties encountered
when attempting to apply TESLA to LWIP in \autoref{sec:lwip}. Then, I
describe the construction of such an interface using TESLA. Finally, I
present an application developed using this method and show the
performance benefits available by applying static analysis.

\subsection{Motivation}

In \autoref{sec:lwip} I investigated how TESLA instrumentation could be applied
to the internal implementation of LWIP, concluding that the most useful place
for instrumentation is actually at the boundary between user and library code.
However, the use of user-registered callbacks means that the library cannot use
TESLA to make assertions on user behaviour.

A solution to this problem should allow users to consume the LWIP libraries as
they would when using the callback API, while also allowing the library to add
TESLA assertions about the behaviour of user code. This would mean the library
is able to enforce temporal safety properties of user programs without any prior
knowledge of the programs.

\subsection{Implementation Strategy}

LWIP distributes a number of example applications that consume the internal TCP
API by using callbacks as described previously---the simplest of these is an
implementation of the echo protocol \cite{RFC0862}. In this section I describe a
modified version of this application that includes TESLA assertions specified by
the library.

In normal usage, a program using the LWIP TCP library calls a particular library
function with a function pointer argument to register their application-specific
callbacks. This is a very flexible approach that gives the program fine-grained
control over how it interacts with the library. \autoref{fig:callbacks} shows
how the program interacts with the library under this usage model.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \node[draw] (call1) {\texttt{callback\textsubscript{1}}};
    \node[draw,below=of call1] (call2) {\texttt{callback\textsubscript{2}}};
    \node[below=of call2] (calletc) {...};
    \node[draw,below=of calletc] (calln) {\texttt{callback\textsubscript{n}}};
    \node[above=of call1] (req) {};
    \node[draw,right=of |(req)(calln),span vertical=(req)(calln)] (lib) {\texttt{liblwip}};
    \node[draw,left=of |(req)(calln),span vertical=(req)(calln)] (prog) {\texttt{program}};

    \draw[-latex] (prog.east |- req.south) -- (req.south -| lib.west);

    \draw[latex-latex] (call1) -- (call1 -| lib.west);
    \draw[latex-latex] (call2) -- (call2 -| lib.west);
    \draw[latex-latex] (calln) -- (calln -| lib.west);

    \draw[-latex] (call1) -- (call1 -| prog.east);
    \draw[-latex] (call2) -- (call2 -| prog.east);
    \draw[-latex] (calln) -- (calln -| prog.east);
  \end{tikzpicture}
  \caption{Default usage of the LWIP callback API}
  \label{fig:callbacks}
\end{figure}

To add TESLA assertions, some of this dynamic behaviour and flexibility must be
forgone. Instead of registering callbacks, user programs must implement a
particular static interface (by implementing functions with names specified by
the library). These functions can then be automatically registered as callbacks
by the library.

However, this is not yet a complete solution. Each TESLA assertion must be
placed at a source location on the execution path it asserts over---intuitively,
this would be within the user-supplied interface functions themselves. Because
the definitions of these functions are not available to the library ahead of time, it must
implement wrapper functions. These wrappers call through to the user-supplied
interface functions, as well as containing the library TESLA assertions.
\autoref{fig:callbacks-tesla} shows how a program interacts with this modified
usage model.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \node[draw] (call1) {\texttt{interface\textsubscript{1}}};
    \node[draw,below=of call1] (call2) {\texttt{interface\textsubscript{2}}};
    \node[below=of call2] (calletc) {...};
    \node[draw,below=of calletc] (calln) {\texttt{interface\textsubscript{n}}};
    \node[above=of call1] (req) {};

    \node[draw,right=of call1] (wrap1) {\texttt{wrapper\textsubscript{1}}};
    \node[draw,right=of call2] (wrap2) {\texttt{wrapper\textsubscript{2}}};
    \node[below=of wrap2] (wrapetc) {...};
    \node[draw,right=of calln] (wrapn) {\texttt{wrapper\textsubscript{n}}};

    \node[draw,dashed,inner sep=0.8em,fit=(wrap1)(wrapn)] (tesla) {};

    \node[draw,right=of |(req)(wrapn),span vertical=(req)(tesla)] (lib) {\texttt{liblwip}};
    \node[draw,left=5em of |(req)(calln),span vertical=(req)(tesla)] (prog) {\texttt{program}};

    \draw[-latex] (prog.east |- req.south) -- (req.south -| lib.west);

    \draw[latex-latex] (call1) -- (wrap1);
    \draw[latex-latex] (call2) -- (wrap2);
    \draw[latex-latex] (calln) -- (wrapn);

    \draw[latex-latex] (wrap1) -- (call1 -| lib.west);
    \draw[latex-latex] (wrap2) -- (call2 -| lib.west);
    \draw[latex-latex] (wrapn) -- (calln -| lib.west);

    \draw[-latex] (call1) -- (call1 -| prog.east);
    \draw[-latex] (call2) -- (call2 -| prog.east);
    \draw[-latex] (calln) -- (calln -| prog.east);

  \end{tikzpicture}
  \caption{Usage of the LWIP callback API with TESLA instrumentation}
  \label{fig:callbacks-tesla}
\end{figure}

Adapting the echo server application to use this model required only that the
wrapper functions and corresponding assertions were written---almost no
modification of the application code was required beyond removing the callback
registration calls.

The result of this adaptation is two further echo server executables---one where
static analysis has been applied to the library assertions, and one where it has
not been.

\subsection{Performance}

I now provide benchmark results for the three different versions of the echo
server (unmodified, instrumented and statically analysed) in order to
demonstrate the performance overhead of both the adapted library interface, and
of TESLA instrumentation.

\subsubsection{Experimental Setup}

A simple benchmark for an echo server is to measure how many requests of a fixed
size it can handle in a fixed time period. To benchmark the three echo server
implementations, an existing benchmarking tool by \textcite{hoyer_rust_2016} was
used. The tool runs for a fixed length of time, sending as many messages to a
server as it can (on a configurable number of threads).

The benchmarks were run on my development machine (Intel Xeon E5-1620
\SI{3.6}{\GHz}, 8 cores, 64GB of RAM) running FreeBSD 11. The benchmarking tool
was run for \SI{60}{\s} with a message size of 512 bytes in every case, and the
number of sending threads was varied from 1 to 10.

For the instrumented and statically analysed versions, the library wrapper code
contained five TESLA assertions covering the possible library calls that the
user code could make. All of these were reported as safe by the model checker.

\subsubsection{Results}

In all three echo servers, throughput was saturated when sending on two or more
threads---using more threads to send data had no effect on throughput. The mean
throughput over 2--10 sending threads was therefore used as a measure of the
maximum possible performance of each server (taken relative to the unmodified
server implementation---the concrete throughput is less interesting than the
proportional overhead). \autoref{fig:echo-bench} compares the relative
throughput for each implementation.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \pgfplotstableread[col sep=comma]{data/echo_bench_avg.dat}\data
    \begin{axis}[
      title={Relative Throughput of Echo Servers},
      ylabel={Throughput relative to unmodified server (\%)},
      xlabel={Server implementation},
      width=0.85\textwidth,
      ybar,
      ymin=0,ymax=110,
      cycle list/Dark2,
      every axis plot/.append style={fill,draw=none,no markers},
      ymajorgrids=true,
      symbolic x coords={Unmodified,Static,Instrumented},
      xtick={Unmodified,Static,Instrumented},
      %x tick label style={rotate=45, anchor=east},
      enlarge x limits=0.25,
      nodes near coords,
      bar shift=0pt,
      bar width=4ex,
    ]
      \addplot+ coordinates { (Unmodified, 100.0) };
      %\addplot+ coordinates { (Static (all), 99.4) };
      \addplot+ coordinates { (Static, 84.1) };
      \addplot+ coordinates { (Instrumented, 60.8) };
    \end{axis}
  \end{tikzpicture}
  \caption{Effect of TESLA instrumentation and library interface adaptation on
  echo server throughput}
  \label{fig:echo-bench}
\end{figure}

The server with runtime TESLA instrumentation achieves only 61\% throughput when
compared to the unmodified version, while the version with static analysis
applied achieves 84\% throughput. There is a significant performance penalty
incurred from using even a small number of TESLA assertions, which can be
improved significantly by applying static analysis to check assertions at
compile time.

Even though all TESLA automata have been removed from the statically analysed
binary, some TESLA code remains in the binary. Using Callgrind
\cite{weidendorfer_tool_2004} (a simulation-based profiling tool) shows that the
statically-analysed server is still spending a large portion of its execution
time in the TESLA runtime library (performing unnecessary work as there are no
automata left to keep track of).

Because the overhead of the remaining TESLA instrumentation is high, even when
there are no automata, I extended the instrumenting tool with an optimisation to
handle this case. The resulting server implementation achieved 99.4\% throughput
compared to the unmodified server. While this performance result is appealing,
it is worth noting that it will only be possible when every automaton is
statically provable.

\subsubsection{Analysis}

It is clear from these results that TESLA instrumentation has a
significant impact on performance. In this section I analyse the causes
of this overhead by using the
\texttt{hwpmc}\footnote{\url{https://www.freebsd.org/cgi/man.cgi?query=hwpmc}}
tools available in FreeBSD.

Hardware performance counters are a set of specialised registers
available on modern processors that can be used to measure architectural
and microarchitectural performance events. For example, the number of
mispredicted branches or the number of cache line misses are commonly
made available. The exact data available varies between architectures,
and even between different models of processor on the same architecture.
FreeBSD makes these performance counters available to user programs
through the \texttt{hwpmc} driver.

Using these counters can have an effect on the performance
characteristics of a program. In order to verify that this effect was
acceptable, the experiment described above was repeated with five
performance counters enabled. There was no significant change in
relative performance when running with counters enabled---the maximum
deviation from the results given above was 1.7\%.\footnote{Absolute
throughput with counters enabled was approximately 90\% of the
throughput with no counters.}

The performance counters enabled were as follows: the number of
instructions retired, the number of load instructions retired, and the
number of loads from the L1, L2 and L3 caches respectively. Each counter
was run in sampling mode, meaning that data was obtained by statistical
sampling over all the events observed. Each server was observed over the
course of a fixed-size data transfer, with the size of the data used
varying from 10--150MB.

An approximation to the amount of work performed by each version is the
total number of instructions retired.\footnote{An instruction is retired
when it has been executed and its effects written back---in a
superscalar architecture, instructions may be speculatively dispatched
but not retired.} \autoref{fig:retired-bench} shows the number of
instructions retired during these transfers for each server version.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      title={Effect of data transfer size on instructions retired},
      width=0.85\textwidth,
      xlabel={Data size (MB)},
      ylabel={Instructions retired},
      xmin=7,xmax=153,
      ymin=0,ymax=250000,
      legend pos=north west,
      ymajorgrids=true,
      grid style=dashed,
      cycle list/Dark2,
      yticklabel style={
	      /pgf/number format/fixed,
	      /pgf/number format/precision=5
      },
      scaled y ticks=false
    ]
      \addplot+[mark=x]
      table [x =size, y =unmod]{data/retired.dat};

      %\addplot+[mark=x]
      %table [x =size, y =all]{data/retired.dat};

      \addplot+[mark=x]
      table [x =size, y =static]{data/retired.dat};

      \addplot+[mark=x]
      table [x =size, y =instr]{data/retired.dat};
      \legend{Unmodified,Static,Instrumented}
    \end{axis}
  \end{tikzpicture}
  \caption{Number of instructions retired during a fixed-size data
  transfer}
  \label{fig:retired-bench}
\end{figure}

It is clear from this data that the number of instructions retired is
proportional to the quantity of data being transferred by the server. On
average, the statically analysed server retired $1.37\times$ the
instructions of the unmodified server, while the instrumented version
retired $2.49\times$ as many. Further experiments with much larger data
files showed that this relationship continued to hold.

The memory access patterns for each version are very similar---for each
one, 28\% of the total instructions retired are load instructions, of
which 94\% are served by the L1 cache, and less than 0.2\% by the L3
cache (although because the total number of instructions executed is
greater, the total number of loads from the L3 cache and cache misses
will increase).

A similar analysis was performed for instruction cache misses, with the
same conclusion---TESLA instrumentation did not significantly change the
microarchitectural performance characteristics of the server
implementations (beyond increasing the number of executed instructions).

The sampled counter data can be used to show where instructions are
spent during execution. \autoref{lst:instr-callchain} shows a sampled
excerpt from the instrumented server. From this data we can see that
TESLA automaton operations account for 37\% of the retired instructions,
with 25\% corresponding to state updates and 12\% to lifetime
management.\footnote{The TESLA terminology for creating and destroying
automata is ``sunrise'' and ``sunset''.}

\begin{listing}[ht]
  \begin{minted}{objdump-nasm}
@ INSTR_RETIRED_ANY [227749 samples]

36.66%  [83489]    strncmp @ /lib/libc.so.7
  100.0%  [83489]     same_static_lifetime @ libtesla.so
    68.99%  [57601]      tesla_update_class_state @ libtesla.so
    16.00%  [13358]      tesla_sunset @ libtesla.so
    15.01%  [12530]      tesla_sunrise @ libtesla.so
06.73%  [15337]    inet_chksum_pseudo @ tesla-app.instr
  ...
  \end{minted}
  \caption{Callchain output showing number of instructions retired with
  TESLA instrumentation enabled}
  \label{lst:instr-callchain}
\end{listing}

The equivalent data for the statically analysed server (in
\autoref{lst:static-callchain}) shows clearly where the remaining
overhead arises. Calls to the automaton lifetime functions remain (with
a similar number of associated instructions), while the state update
functions have been removed. Calls to the lifetime functions can be
removed only if there are no automata to be instrumented.

\begin{listing}[ht]
  \begin{minted}{objdump-nasm}
@ INSTR_RETIRED_ANY [125375 samples]

22.71%  [28467]    strncmp @ /lib/libc.so.7
  100.0%  [28467]     same_static_lifetime @ libtesla.so
    51.17%  [14567]      tesla_sunrise @ libtesla.so
    48.83%  [13900]      tesla_sunset @ libtesla.so
12.74%  [15968]    inet_chksum_pseudo @ tesla-app.static
  ...
  \end{minted}
  \caption{Callchain output showing number of instructions retired with
  TESLA instrumentation removed}
  \label{lst:static-callchain}
\end{listing}

\subsection{Summary}

In this section I have shown how TESLA can be used by library developers to
apply temporal assertions to users of the library without prior knowledge of the
user code. Additionally, I have demonstrated a method for adapting library
interfaces not explicitly designed with TESLA instrumentation in mind. This
technique was applied to an application using the LWIP TCP library,
demonstrating the performance improvements achievable by using static
analysis. Finally, I have given an analysis of the performance overheads
observed that shows TESLA instrumentation is indeed the source.

The methods developed have some shortcomings when compared to typical C library
development:
\begin{description}
  \item[Distribution] A library developed using these methods must be
    distributed as LLVM bitcode together with the associated TESLA
    manifest---this means that users must install the TESLA toolchain and adapt
    their build process to use the library.

  \item[Safety] TESLA has no way of preventing users from performing unsafe
    operations outsidof the library code. For example, if a library asserts that
    a lock is held before a logging function is called, nothing stops the user
    from making calls to \mintinline{c}{fprintf} without holding the lock.

  \item[Performance] If the user code is amenable to static analysis, then the
    performance impact of TESLA should be minimal. However, it is easily
    conceivable that users will not write code in this way (possibly leading to
    performance issues).
\end{description}

Despite these issues, this is still a useful tool available to library
developers for preventing bugs in user code. Additionally, it represents a more
general use of TESLA than previous work.
